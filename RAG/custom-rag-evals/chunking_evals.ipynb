{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba0075b2-0479-43f6-9ce2-5bd2cc3b1bd7",
   "metadata": {},
   "source": [
    "# Evaluating Domain Specific RAG Chunking & Embedding Strategies\n",
    "\n",
    "The first step of creating RAG systems, choosing how to load and split your documents, is an often overlooked yet critical step. Recent research from [ChromaDB](https://trychroma.com) titled [Evaluating Chunking Strategies for Retrieval](https://research.trychroma.com/evaluating-chunking) outlines various popular chunking approaches and a few novel ideas to help give a good direction of choosing your chunking strategy.\n",
    "\n",
    "<img src=\"./media_2/hero_table.png\" width=\"600\">\n",
    "\n",
    "Their main findings while using `text-embedding-3-large` from OpenAI:\n",
    "1. The **Cluster Semantic Chunker** with a 200 token chunk size achieves the highest precision, precision with perfect recall, and intersection over union.\n",
    "2. The **LLM Chunker** achieves the highest recall.\n",
    "3. The **Recursive Character Text Splitter** with chunk size 200 achieves consistently high metrics and is a good lightweight option.\n",
    "\n",
    "I've broken down how each one of these chunking strategies works [in a prior notebook](https://github.com/ALucek/chunking-strategies) using their [respective repo](https://github.com/brandonstarxel/chunking_evaluation). On top of the different chunking implementations, Chroma provided their evaluation framework that allows you to run tests both on a standard and domain specific documents to determine what chunking and embedding method might be the best for your specific application. While following the research data is a useful start, running your own experiments can help you find exactly what works best for you.\n",
    "\n",
    "We'll be covering the four main approaches to:\n",
    "\n",
    "1. Create Custom Chunking Strategies\n",
    "2. Evaluate Custom & Existing Chunking Strategies\n",
    "3. Evaluate Custom & Existing Embedding Strategies\n",
    "4. Create a Synthetic Dataset for Domain Specific Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca262986-755f-4c8f-a73b-d755cb0d0234",
   "metadata": {},
   "source": [
    "#### Installing the [Chunking Evaluation Repo](https://github.com/brandonstarxel/chunking_evaluation/tree/main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c91ca6b6-f2b5-4499-bf29-4943701fef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/brandonstarxel/chunking_evaluation.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d605462-6246-444b-855c-d45217fa3d7a",
   "metadata": {},
   "source": [
    "**Imports & Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c275a44e-6dd1-4422-a255-47fe5fb13633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunking_evaluation import GeneralEvaluation, SyntheticEvaluation, BaseChunker\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display, clear_output\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455d2ae2-d01f-414d-95fb-27f8b43b3bd7",
   "metadata": {},
   "source": [
    "---\n",
    "# Custom Chunking Strategies\n",
    "\n",
    "We'll use the `BaseChunker` class to define our own. At it's core `BaseChunker` is very simple:\n",
    "\n",
    "```python\n",
    "\n",
    "class BaseChunker(ABC):\n",
    "    @abstractmethod\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        pass\n",
    "\n",
    "```\n",
    "\n",
    "Expecting only a `split_text` method that can take in a string and return a list of strings, which is our chunks. The transformation along the way can be more creatively defined. \n",
    "\n",
    "As an example, we'll define a `SentenceChunker` that uses a simple regex to attempt to split text at a basic sentence level, with one variable that controls how many sentences are included in each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "094ae940-b02d-4570-836d-f2202fec8ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceChunker(BaseChunker):\n",
    "    def __init__(self, sentences_per_chunk: int = 3):\n",
    "        # Initialize the chunker with the number of sentences per chunk\n",
    "        self.sentences_per_chunk = sentences_per_chunk\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        # Handle the case where the input text is empty\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        # Split the input text into sentences using regular expression\n",
    "        # Regex looks for white space following . ! or ? and makes a split\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        chunks = []\n",
    "\n",
    "        # Group sentences into chunks based on the specified number\n",
    "        for i in range(0, len(sentences), self.sentences_per_chunk):\n",
    "            # Combine sentences into a single chunk\n",
    "            chunk = ' '.join(sentences[i:i + self.sentences_per_chunk])\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        # Return the list of chunks\n",
    "        return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ae9b3b-7c6c-43fb-9a58-7c0bdc30d938",
   "metadata": {},
   "source": [
    "**Loading Example Document**\n",
    "\n",
    "We'll be using [NVIDIA's Form 10-K for FY24](https://d18rn0p25nwr6d.cloudfront.net/CIK-0001045810/1cbe8fe7-e08a-46e3-8dcc-b429fc06c1a4.pdf) as an example, converted into a plain text file already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b88d6e7f-dc54-4c78-8586-4b00d645bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./domain_specific/nvidia_10k.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    nvidia_10k = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccd03ce-be31-4109-b399-1dce05306478",
   "metadata": {},
   "source": [
    "**Chunking the Document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fbaa147-d355-4373-bce1-67b366416fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SentenceChunker\n",
    "sentence_chunker = SentenceChunker(sentences_per_chunk = 10)\n",
    "\n",
    "# Split the Document\n",
    "sentence_chunks = sentence_chunker.split_text(nvidia_10k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf6b079-8ebf-49e4-b6da-52c78c79348f",
   "metadata": {},
   "source": [
    "**Observing the Chunk Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "156ad35a-eb79-41e2-8fa6-956195257522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b713e4f9-981c-44b3-adf4-e92c762ae742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Headquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998. Our Businesses\\nWe report our business results in two segments. The Compute & Networking segment is comprised of our Data Center accelerated computing platforms and end-to-end networking platforms including Quantumfor InfiniBand and Spectrum for Ethernet; our NVIDIA DRIVE automated-driving platform and automotive development agreements; Jetson robotics and other\\nembedded platforms; NVIDIA AI Enterprise and other software; and DGX Cloud software and services. The Graphics segment includes GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure; Quadro/NVIDIARTX GPUs for enterprise workstation graphics; virtual GPU, or vGPU, software for cloud-based visual and virtual computing; automotive platforms for\\ninfotainment systems; and Omniverse Enterprise software for building and operating metaverse and 3D internet applications. Our Markets\\nWe specialize in markets where our computing platforms can provide tremendous acceleration for applications. These platforms incorporate processors,interconnects, software, algorithms, systems, and services to deliver unique value. Our platforms address four large markets where our expertise is critical: DataCenter, Gaming, Professional Visualization, and Automotive. Data Center\\nThe NVIDIA Data Center platform is focused on accelerating the most compute-intensive workloads, such as AI, data analytics, graphics and scientific\\ncomputing, delivering significantly better performance and power efficiency relative to conventional CPU-only approaches. It is deployed in cloud, hyperscale,on-premises and edge data centers. The platform consists of compute and networking offerings typically delivered to customers as systems, subsystems, ormodules, along with software and services.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_chunks[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43b1ef9-80ed-48c0-84b0-f4d264891639",
   "metadata": {},
   "source": [
    "**Now that we have our text corpus, we can run experiments to determine how well it works. First, we need to define our metrics of interest to measure.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee457e0-ab33-408f-9eb7-a94b37aea3eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "# Metrics Breakdown\n",
    "\n",
    "The built in metrics here are slightly different from traditional information retrieval metrics, which usually operate a document level. These will be more concerned with measuring the token level performance of our chunking and embedding strategies. The motivation for this is that:\n",
    "\n",
    "*For a given query related to a specific corpus, only a subset of tokens within that corpus will be relevant. Ideally, for both efficiency and accuracy, the retrieval system should retrieve exactly and only the relevant tokens for each query across the entire corpus.*\n",
    "\n",
    "This is better suited for testing chunking as a retrieval part of RAG systems, as we are less concerned with the specific document than the actual relevant token level information for the LLM to process, trying to maximize the relevant tokens and exclude irrelevant, redundant, and distracting superfluous information.\n",
    "\n",
    "<img src=\"./media_2/recall_precision.png\" width=800>\n",
    "\n",
    "## Variables\n",
    "\n",
    "- $q$ represents a specific query\n",
    "- $\\mathbf{C}$ represents the chunked corpus (the entire document split into chunks)  \n",
    "- $t_e$ represents the set of tokens in relevant excerpts/highlights (ground truth)  \n",
    "- $t_r$ represents the set of tokens in retrieved chunks (what our system returns)  \n",
    "\n",
    "A **highlight** is a segment of text in the original document that contains the relevant information needed to answer a specific query. Highlights serve as the \"ground truth\" against which we measure our chunking and retrieval performance.  \n",
    "\n",
    "For example:\n",
    "\n",
    "- **Document**: \"The Sun is composed primarily of hydrogen and helium. Through nuclear fusion in its core, it converts hydrogen into helium, releasing massive amounts of energy. This energy travels to Earth as sunlight and heat.\"\n",
    "- **Query**: \"How does the Sun produce energy?\"\n",
    "- **Highlight**: \"Through nuclear fusion in its core, it converts hydrogen into helium, releasing massive amounts of energy.\"\n",
    "\n",
    "## Recall\n",
    "\n",
    "$\\text{Recall}_q(\\mathbf{C}) = \\frac{|t_e \\cap t_r|}{|t_e|}$\n",
    "\n",
    "**Calculated by**: length of overlap between retrieved chunks and highlights / total length of highlights\n",
    "\n",
    "Measures what fraction of the important/relevant text is captured by the retrieved chunks. Ranges from 0 to 1, where 1 means all relevant text was captured. A low recall means the chunking strategy is missing important information.\n",
    "\n",
    "**Answers**: How much of these important highlighted segments did we capture?\n",
    "\n",
    "**Example**: If a highlight is 100 tokens and our chunks only capture 70 tokens of it, recall = 0.7\n",
    "\n",
    "## Precision\n",
    "\n",
    "$\\text{Precision}_q(\\mathbf{C}) = \\frac{|t_e \\cap t_r|}{|t_r|}$\n",
    "\n",
    "**Calculated by**: length of overlap between retrieved chunks and highlights / total length of retrieved chunks\n",
    "\n",
    "Measures how much of the retrieved text is actually relevant. Ranges from 0 to 1, where 1 means all retrieved text was relevant. A low precision means the chunks contain a lot of irrelevant text.\n",
    "\n",
    "**Answers**: How much of what we retrieved matches these highlights?\n",
    "\n",
    "**Example**: If we retrieve 200 tokens of text but only 70 overlap with highlights, precision = 0.35\n",
    "\n",
    "## Precision Ω\n",
    "\n",
    "$\\text{Precision}_\\Omega(\\mathbf{C}) = \\frac{|t_e \\cap t_r|}{|t_r| + |t_e \\setminus t_r|}$\n",
    "\n",
    "Measures precision in an ideal scenario where all relevant text is captured. Shows the theoretical best precision possible for a given chunking strategy. Like regular precision but assumes you've retrieved all highlights. Lower precision omega means chunks are inherently too large or poorly aligned with natural text boundaries.\n",
    "\n",
    "**Answers**: If we made sure to get all the highlights, how precise could we be?\n",
    "\n",
    "**Example**: If a chunking strategy always creates chunks twice as large as needed, precision omega would be around 0.5\n",
    "\n",
    "## Intersection over Union (IoU)\n",
    "\n",
    "$\\text{IoU}_q(\\mathbf{C}) = \\frac{|t_e \\cap t_r|}{|t_e| + |t_r| - |t_e \\cap t_r|}$\n",
    "\n",
    "**Calculated by**: length of overlap / length of union of retrieved chunks and highlights\n",
    "\n",
    "Balances both precision and recall in a single metric. Ranges from 0 to 1, where 1 is perfect overlap. A low IoU indicates either missing content (poor recall) or retrieving too much irrelevant text (poor precision). IoU penalizes missing important content and including irrelevant content while handling redundant information.\n",
    "\n",
    "**Answers**: How well do our retrieved chunks overlap with these highlights overall?\n",
    "\n",
    "**Example**: If we retrieve 200 tokens, the highlight is 100 tokens, and overlap is 70 tokens, IoU = 70/(200+100-70) = 0.304\n",
    "\n",
    "## Metric Interpretation\n",
    "\n",
    "These metrics work well together:\n",
    "- High recall + low precision = retrieving too much text\n",
    "- Low recall + high precision = missing important content\n",
    "- High IoU = good balance of both\n",
    "- Precision Ω helps evaluate the chunking strategy independent of the retrieval step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8048fa-a83f-4deb-a63e-f12eda5269f3",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluating Chunking Strategies and Embedding Models\n",
    "\n",
    "Built into the repo is a default evaluation structure of 5 text documents and respective question & highlights data. The text corpus includes a mix of clean and unstructured text documents to simulate various text chunking and retrieval scenarios. These include:\n",
    "\n",
    "1. [State of the Union 2024](https://www.whitehouse.gov/state-of-the-union-2024/): A clean, well-structured transcript of the 2024 presidential address (10,444 tokens)\n",
    "2. [Wikitext](https://huggingface.co/datasets/Salesforce/wikitext): A curated collection of high-quality Wikipedia articles from verified Good and Featured sections (26,649 tokens subset)\n",
    "3. [UltraChat 200k](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k): A dataset of ChatGPT-generated dialogues with JSON syntax intact to simulate real-world messy data (7,727 tokens subset)\n",
    "4. [ConvFinQA](https://github.com/czyssrs/ConvFinQA): A conversational Q&A dataset focused on numerical reasoning in financial reports (166,177 tokens subset)\n",
    "5. [PMC Open Access](https://huggingface.co/datasets/pmc/open_access): Biomedical and life sciences journal literature from the National Library of Medicine's open access collection (117,211 tokens subset)\n",
    "\n",
    "A standard set of question and highlights have been generated and filtered as well, [full file viewable here](https://github.com/brandonstarxel/chunking_evaluation/blob/main/chunking_evaluation/evaluation_framework/general_evaluation_data/questions_df.csv)\n",
    "\n",
    "<img src=\"./media_2/standard_eval_corpus.png\" width=800>\n",
    "\n",
    "The question/references take the form of, for example:\n",
    "\n",
    "- **Question**: `What were the values of other indefinite-lived intangible assets at the end of 2011 and 2012?`\n",
    "- **Reference(s)**: `[{\"content\": \"other indefinite-lived intangible assets were $132 million and $174 million at december 31, 2012 and 2011, respectively, and principally included registered trademarks\",\"start_index\": 568963,\"end_index\": 569130}]`\n",
    "\n",
    "Which show the respective text chunk, and its character position within the entire database collection.\n",
    "\n",
    "## General Evaluation Process\n",
    "\n",
    "The general evaluation will take the text, chunk it using the chosen chunker along with the chunks start and end index, then:\n",
    "\n",
    "- **Calculate Retrieval Performance**:\n",
    "    1. Embed the evaluation questions using the chosen embedding function\n",
    "    2. Perform vector similarity search to retrieve top-k most relevant chunks per question\n",
    "    3. Calculates regular metrics:\n",
    "        1. *Recall*: How much of the highlighted segments were captured\n",
    "        2. *Precision*: How much of the retrieved chunks were actually relevant\n",
    "        3. *IoU*: Overall balance of precision and recall\n",
    "- **Calculate Precision Ω Performance**:\n",
    "  1. Examine ALL chunks in the collection\n",
    "  2. Identify which chunks contain any part of the highlight segments\n",
    "  3. Calculate theoretical best precision possible if you retrieved all necessary chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93590001-0c41-453f-a5f6-2eefe849c07b",
   "metadata": {},
   "source": [
    "**Start General Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb30b56-b9e6-4a56-9274-ae7675e988a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instantiate the General Eval\n",
    "evaluation = GeneralEvaluation()\n",
    "\n",
    "# Define Chunking Approach\n",
    "sentence_chunker = SentenceChunker(sentences_per_chunk = 10)\n",
    "\n",
    "# Define OpenAI Embedding Model\n",
    "default_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model_name=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "# Run the Eval With Chunker and Embedding Model\n",
    "results = evaluation.run(sentence_chunker, default_ef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf2e5b-239f-4c45-b09d-63b32a0d549a",
   "metadata": {},
   "source": [
    "**Observing Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "256c83c1-a8e4-4e90-af98-ac96f20a3392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "def print_metrics(results):\n",
    "    \n",
    "    # Grab Summary Metrics    \n",
    "    metrics = {\n",
    "        'Recall': (results['recall_mean'], results['recall_std']),\n",
    "        'Precision': (results['precision_mean'], results['precision_std']),\n",
    "        'Precision Ω': (results['precision_omega_mean'], results['precision_omega_std']),\n",
    "        'IoU': (results['iou_mean'], results['iou_std'])\n",
    "    }\n",
    "    \n",
    "    # Print each metric with mean ± std\n",
    "    for metric, (mean, std) in metrics.items():\n",
    "        print(f\"{metric}: {mean:.4f} ± {std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "855000c9-b3c8-4f2b-be5b-d1db564b4033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.8703 ± 0.3216\n",
      "Precision: 0.0370 ± 0.0303\n",
      "Precision Ω: 0.1674 ± 0.1084\n",
      "IoU: 0.0370 ± 0.0303\n"
     ]
    }
   ],
   "source": [
    "print_metrics(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580dadfa-9b35-4f83-adec-627c7c9ae315",
   "metadata": {},
   "source": [
    "**Interpretation**: The 10 sentence chunking strategy demonstrated strong recall performance (87.03% ± 32.16%), indicating effective retrieval of relevant information. However, the low precision (3.70% ± 3.03%) and IoU (3.70% ± 3.03%) metrics suggest significant inclusion of irrelevant text within chunks. The precision Ω value of 16.74% ± 10.84% indicates that even under optimal retrieval conditions, the chunking strategy includes substantial extraneous content. When compared to benchmark chunkers (above), while the recall performance aligns with state-of-the-art approaches (83-91%), the precision metrics suggest opportunities for improvement through reduced chunk sizes or refined boundary determination methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7011e8c-9c49-48d4-82ea-0476f4d95a28",
   "metadata": {},
   "source": [
    "### Embedding Functions\n",
    "\n",
    "We've mostly focused on the chunking strategies, but along with this is the ability to plug in and out different embedding functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122eed23-176b-4fd5-a18b-703c43d2a8be",
   "metadata": {},
   "source": [
    "**Existing Integrations with ChromaDB**\n",
    "\n",
    "As we demonstrated in the above example, you can easily use already built in [embedding functions](https://github.com/chroma-core/chroma/tree/main/chromadb/utils/embedding_functions) from Chroma's repo.\n",
    "\n",
    "Let's demonstrate this by using the [Sentence Transformers Embedding Function](https://github.com/chroma-core/chroma/blob/main/chromadb/utils/embedding_functions/sentence_transformer_embedding_function.py) with the [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dbb0407a-8732-4fb8-9762-98a07d3304bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.7859 ± 0.3897\n",
      "Precision: 0.0335 ± 0.0306\n",
      "Precision Ω: 0.1674 ± 0.1084\n",
      "IoU: 0.0334 ± 0.0306\n"
     ]
    }
   ],
   "source": [
    "# Load Embedding Function\n",
    "st_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Same Chunking Strategy\n",
    "sentence_chunker = SentenceChunker(sentences_per_chunk = 10)\n",
    "\n",
    "# Run the Eval With Chunker and Ollama Embedding Function\n",
    "st_results = evaluation.run(sentence_chunker, st_ef)\n",
    "\n",
    "# Display Results\n",
    "print_metrics(st_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfe8fab-7a77-4126-a36c-50c2711ffe4d",
   "metadata": {},
   "source": [
    "**Interpretation**: The lightweight open source model decreased performance over OpenAI's SoTA model. To be expected, and proven!\n",
    "\n",
    "If you have custom embedding functions, i.e. if you're applying something like a ([query only linear adapter](https://github.com/ALucek/linear-adapter-embedding) to your embeddings), or have a custom fine tuned model, you can easily create your own compatible embedding function with the following outline:\n",
    "\n",
    "```python\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        # embed the documents somehow\n",
    "        return embeddings\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc57ca6-f13d-40da-b008-b3a056799136",
   "metadata": {},
   "source": [
    "## Running Multiple Evaluations At Once\n",
    "\n",
    "When looking for the optimal configuration for chunking and embedding, you may want to run a hyperparameter sweep to test various setups. This is made a lot easier with this framework, let's run a simple sweep now across multiple `SentenceChunker` configurations and embedding functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41579aac-350e-4e7c-b517-b95243cf6fae",
   "metadata": {},
   "source": [
    "**Defining our Chunkers and Embedding Models**\n",
    "\n",
    "*Note: You don't necessarily have to just use one chunker here, you could load this up with multiple kinds of chunkers and configurations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "741b8a99-afb7-42fe-91ec-e6bfbcce88db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our Configurations\n",
    "chunkers = [\n",
    "    SentenceChunker(sentences_per_chunk = 5),\n",
    "    SentenceChunker(sentences_per_chunk = 10),\n",
    "    SentenceChunker(sentences_per_chunk = 15),\n",
    "    SentenceChunker(sentences_per_chunk = 20),\n",
    "]\n",
    "\n",
    "# Defining our Embedding Functions\n",
    "embedders = [\n",
    "    embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\"),\n",
    "    embedding_functions.OpenAIEmbeddingFunction(api_key=os.environ[\"OPENAI_API_KEY\"], model_name=\"text-embedding-3-large\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6854c7f-8da3-4bf0-962c-23dc85c6ba68",
   "metadata": {},
   "source": [
    "**Main Sweep**\n",
    "\n",
    "Logic is going to run each embedding function with each chunker setup and create a final dataframe of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "03cc8d02-56f7-438e-8805-46a46d34d876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iou_mean</th>\n",
       "      <th>iou_std</th>\n",
       "      <th>recall_mean</th>\n",
       "      <th>recall_std</th>\n",
       "      <th>precision_omega_mean</th>\n",
       "      <th>precision_omega_std</th>\n",
       "      <th>precision_mean</th>\n",
       "      <th>precision_std</th>\n",
       "      <th>chunker</th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>embedding_function</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.060672</td>\n",
       "      <td>0.055114</td>\n",
       "      <td>0.751719</td>\n",
       "      <td>0.404472</td>\n",
       "      <td>0.294542</td>\n",
       "      <td>0.176066</td>\n",
       "      <td>0.061171</td>\n",
       "      <td>0.055477</td>\n",
       "      <td>SentenceChunker</td>\n",
       "      <td>5</td>\n",
       "      <td>SentenceTransformerEmbeddingFunction</td>\n",
       "      <td>SentenceChunker_5_SentenceTransformerEmbedding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.069801</td>\n",
       "      <td>0.054571</td>\n",
       "      <td>0.871479</td>\n",
       "      <td>0.306642</td>\n",
       "      <td>0.294542</td>\n",
       "      <td>0.176066</td>\n",
       "      <td>0.070187</td>\n",
       "      <td>0.054913</td>\n",
       "      <td>SentenceChunker</td>\n",
       "      <td>5</td>\n",
       "      <td>OpenAIEmbeddingFunction</td>\n",
       "      <td>SentenceChunker_5_OpenAIEmbeddingFunction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.033444</td>\n",
       "      <td>0.030564</td>\n",
       "      <td>0.785924</td>\n",
       "      <td>0.389651</td>\n",
       "      <td>0.167426</td>\n",
       "      <td>0.108418</td>\n",
       "      <td>0.033532</td>\n",
       "      <td>0.030637</td>\n",
       "      <td>SentenceChunker</td>\n",
       "      <td>10</td>\n",
       "      <td>SentenceTransformerEmbeddingFunction</td>\n",
       "      <td>SentenceChunker_10_SentenceTransformerEmbeddin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.036992</td>\n",
       "      <td>0.030295</td>\n",
       "      <td>0.870283</td>\n",
       "      <td>0.321624</td>\n",
       "      <td>0.167426</td>\n",
       "      <td>0.108418</td>\n",
       "      <td>0.037043</td>\n",
       "      <td>0.030341</td>\n",
       "      <td>SentenceChunker</td>\n",
       "      <td>10</td>\n",
       "      <td>OpenAIEmbeddingFunction</td>\n",
       "      <td>SentenceChunker_10_OpenAIEmbeddingFunction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.021538</td>\n",
       "      <td>0.019466</td>\n",
       "      <td>0.766879</td>\n",
       "      <td>0.403846</td>\n",
       "      <td>0.120034</td>\n",
       "      <td>0.082415</td>\n",
       "      <td>0.021577</td>\n",
       "      <td>0.019499</td>\n",
       "      <td>SentenceChunker</td>\n",
       "      <td>15</td>\n",
       "      <td>SentenceTransformerEmbeddingFunction</td>\n",
       "      <td>SentenceChunker_15_SentenceTransformerEmbeddin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.024965</td>\n",
       "      <td>0.020593</td>\n",
       "      <td>0.882058</td>\n",
       "      <td>0.314057</td>\n",
       "      <td>0.120034</td>\n",
       "      <td>0.082415</td>\n",
       "      <td>0.024979</td>\n",
       "      <td>0.020606</td>\n",
       "      <td>SentenceChunker</td>\n",
       "      <td>15</td>\n",
       "      <td>OpenAIEmbeddingFunction</td>\n",
       "      <td>SentenceChunker_15_OpenAIEmbeddingFunction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.015787</td>\n",
       "      <td>0.015199</td>\n",
       "      <td>0.738888</td>\n",
       "      <td>0.428088</td>\n",
       "      <td>0.093078</td>\n",
       "      <td>0.063231</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>0.015205</td>\n",
       "      <td>SentenceChunker</td>\n",
       "      <td>20</td>\n",
       "      <td>SentenceTransformerEmbeddingFunction</td>\n",
       "      <td>SentenceChunker_20_SentenceTransformerEmbeddin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.018720</td>\n",
       "      <td>0.015482</td>\n",
       "      <td>0.862835</td>\n",
       "      <td>0.333720</td>\n",
       "      <td>0.093078</td>\n",
       "      <td>0.063231</td>\n",
       "      <td>0.018729</td>\n",
       "      <td>0.015486</td>\n",
       "      <td>SentenceChunker</td>\n",
       "      <td>20</td>\n",
       "      <td>OpenAIEmbeddingFunction</td>\n",
       "      <td>SentenceChunker_20_OpenAIEmbeddingFunction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iou_mean   iou_std  recall_mean  recall_std  precision_omega_mean  \\\n",
       "0  0.060672  0.055114     0.751719    0.404472              0.294542   \n",
       "1  0.069801  0.054571     0.871479    0.306642              0.294542   \n",
       "2  0.033444  0.030564     0.785924    0.389651              0.167426   \n",
       "3  0.036992  0.030295     0.870283    0.321624              0.167426   \n",
       "4  0.021538  0.019466     0.766879    0.403846              0.120034   \n",
       "5  0.024965  0.020593     0.882058    0.314057              0.120034   \n",
       "6  0.015787  0.015199     0.738888    0.428088              0.093078   \n",
       "7  0.018720  0.015482     0.862835    0.333720              0.093078   \n",
       "\n",
       "   precision_omega_std  precision_mean  precision_std          chunker  \\\n",
       "0             0.176066        0.061171       0.055477  SentenceChunker   \n",
       "1             0.176066        0.070187       0.054913  SentenceChunker   \n",
       "2             0.108418        0.033532       0.030637  SentenceChunker   \n",
       "3             0.108418        0.037043       0.030341  SentenceChunker   \n",
       "4             0.082415        0.021577       0.019499  SentenceChunker   \n",
       "5             0.082415        0.024979       0.020606  SentenceChunker   \n",
       "6             0.063231        0.015800       0.015205  SentenceChunker   \n",
       "7             0.063231        0.018729       0.015486  SentenceChunker   \n",
       "\n",
       "   chunk_size                    embedding_function  \\\n",
       "0           5  SentenceTransformerEmbeddingFunction   \n",
       "1           5               OpenAIEmbeddingFunction   \n",
       "2          10  SentenceTransformerEmbeddingFunction   \n",
       "3          10               OpenAIEmbeddingFunction   \n",
       "4          15  SentenceTransformerEmbeddingFunction   \n",
       "5          15               OpenAIEmbeddingFunction   \n",
       "6          20  SentenceTransformerEmbeddingFunction   \n",
       "7          20               OpenAIEmbeddingFunction   \n",
       "\n",
       "                                              config  \n",
       "0  SentenceChunker_5_SentenceTransformerEmbedding...  \n",
       "1          SentenceChunker_5_OpenAIEmbeddingFunction  \n",
       "2  SentenceChunker_10_SentenceTransformerEmbeddin...  \n",
       "3         SentenceChunker_10_OpenAIEmbeddingFunction  \n",
       "4  SentenceChunker_15_SentenceTransformerEmbeddin...  \n",
       "5         SentenceChunker_15_OpenAIEmbeddingFunction  \n",
       "6  SentenceChunker_20_SentenceTransformerEmbeddin...  \n",
       "7         SentenceChunker_20_OpenAIEmbeddingFunction  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize Evaluation and Results Storage\n",
    "evaluation = GeneralEvaluation()\n",
    "results = []\n",
    "\n",
    "# Helper Function\n",
    "def get_config_name(chunker, ef):\n",
    "    chunk_size = chunker.sentences_per_chunk if hasattr(chunker, 'sentences_per_chunk') else 0\n",
    "    ef_name = ef.model_name if hasattr(ef, 'model_name') else ef.__class__.__name__\n",
    "    return f\"{chunker.__class__.__name__}_{chunk_size}_{ef_name}\"\n",
    "\n",
    "# Progress tracking\n",
    "total_combinations = len(chunkers) * len(embedders)\n",
    "current_combination = 0\n",
    "\n",
    "# Run evaluation sweep\n",
    "for chunker in chunkers:\n",
    "    for ef in embedders:\n",
    "        current_combination += 1\n",
    "        try:\n",
    "            print(f\"Evaluating combination {current_combination}/{total_combinations}:\")\n",
    "            print(f\"  Chunker: {chunker.__class__.__name__} (size: {chunker.sentences_per_chunk})\")\n",
    "            print(f\"  Embedding: {ef.model_name if hasattr(ef, 'model_name') else ef.__class__.__name__}\")\n",
    "            \n",
    "            # Run evaluation\n",
    "            result = evaluation.run(chunker, ef, retrieve=5)\n",
    "            \n",
    "            # Clean up and store results\n",
    "            if 'corpora_scores' in result:\n",
    "                del result['corpora_scores']\n",
    "            \n",
    "            # Add configuration identifiers\n",
    "            result['chunker'] = chunker.__class__.__name__\n",
    "            result['chunk_size'] = chunker.sentences_per_chunk\n",
    "            result['embedding_function'] = ef.model_name if hasattr(ef, 'model_name') else ef.__class__.__name__\n",
    "            result['config'] = get_config_name(chunker, ef)\n",
    "            \n",
    "            results.append(result)\n",
    "            clear_output(wait=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Error Handling Just in Case\n",
    "            print(f\"Error in combination {current_combination}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Create final DataFrame and display\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\nFinal Results:\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3563a635-f7f5-4c92-a9d5-2ab3efec503d",
   "metadata": {},
   "source": [
    "<img src=\"./media_2/sweep_graphs.png\" width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e25f25-93a4-4664-ba44-0b1352cca9d0",
   "metadata": {},
   "source": [
    "---\n",
    "# Domain Specific Evaluation Pipelines\n",
    "\n",
    "While general evals are great for getting up and running, it's more than likely that you're looking for the best chunking strategy and embedding model combination for your own specific documentation.\n",
    "\n",
    "Chroma also open sourced their methodology for generating the dataset of questions and chunks from a text corpus automatically in their [synthetic_evaluation](https://github.com/brandonstarxel/chunking_evaluation/blob/main/chunking_evaluation/evaluation_framework/synthetic_evaluation.py) framework, allowing you to generate evaluation datasets tailored to your domain. \n",
    "\n",
    "The pipeline works by:\n",
    "1. Randomly selecting segments (4000 characters) from your input documents\n",
    "2. Using GPT-4 to generate natural questions based on the content, along with relevant supporting references\n",
    "3. Identifying and extracting precise text spans that contain the information needed to answer each question\n",
    "4. Filter for duplicates and similarity to remove redundant or unrelated questions.\n",
    "\n",
    "Within this there are two reference extraction methods available:\n",
    "1. **Exact matching**: Finds precise text spans in the source document\n",
    "2. **Approximate matching**: Pre-chunks text into 100-character segments and allows references to span multiple chunks\n",
    "\n",
    "Let's apply this with our NVIDIA Form 10-K from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4a9039b5-0611-46e1-a0de-12ef13ad2adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the corpora paths, you can have multiple but we'll just use our one file\n",
    "corpora_paths = [\n",
    "    './domain_specific/nvidia_10k.txt',\n",
    "]\n",
    "csv_path = './domain_specific/generated_queries_and_excerpts.csv'\n",
    "\n",
    "# Initialize the evaluation\n",
    "synthetic_pipeline = SyntheticEvaluation(corpora_paths, csv_path, openai_api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ce9d6b24-37c4-4e8a-80ef-2f2aadd80ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>references</th>\n",
       "      <th>corpus_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the net income of NVIDIA Corporation f...</td>\n",
       "      <td>[{\"content\": \"tax expense (benefit) 4,058 (187...</td>\n",
       "      <td>./domain_specific/nvidia_10k.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the implications of failing to comply...</td>\n",
       "      <td>[{\"content\": \"Administration of China, or CAC....</td>\n",
       "      <td>./domain_specific/nvidia_10k.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the consequences of not adhering to d...</td>\n",
       "      <td>[{\"content\": \"ct to penalties of up to \\u20ac2...</td>\n",
       "      <td>./domain_specific/nvidia_10k.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the steps involved in revenue recogni...</td>\n",
       "      <td>[{\"content\": \"share.\\nRevenue Recognition\\nWe ...</td>\n",
       "      <td>./domain_specific/nvidia_10k.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How are operating lease assets and liabilities...</td>\n",
       "      <td>[{\"content\": \"lease payments over the lease te...</td>\n",
       "      <td>./domain_specific/nvidia_10k.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is the net income of NVIDIA Corporation f...   \n",
       "1  What are the implications of failing to comply...   \n",
       "2  What are the consequences of not adhering to d...   \n",
       "3  What are the steps involved in revenue recogni...   \n",
       "4  How are operating lease assets and liabilities...   \n",
       "\n",
       "                                          references  \\\n",
       "0  [{\"content\": \"tax expense (benefit) 4,058 (187...   \n",
       "1  [{\"content\": \"Administration of China, or CAC....   \n",
       "2  [{\"content\": \"ct to penalties of up to \\u20ac2...   \n",
       "3  [{\"content\": \"share.\\nRevenue Recognition\\nWe ...   \n",
       "4  [{\"content\": \"lease payments over the lease te...   \n",
       "\n",
       "                          corpus_id  \n",
       "0  ./domain_specific/nvidia_10k.txt  \n",
       "1  ./domain_specific/nvidia_10k.txt  \n",
       "2  ./domain_specific/nvidia_10k.txt  \n",
       "3  ./domain_specific/nvidia_10k.txt  \n",
       "4  ./domain_specific/nvidia_10k.txt  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_df = pd.read_csv(csv_path)\n",
    "synthetic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5a039c-a684-4c3c-9f72-282040c256a5",
   "metadata": {},
   "source": [
    "**Run the Data Generation Pipeline**\n",
    "\n",
    "The `generate_queries_and_excerpts` method takes the arguments:\n",
    "1. **approximate_excerpts**: Whether or not to use the chunked flexible approach or exact text matching\n",
    "2. **num_rounds**: How many times per document to run query generations\n",
    "3. **queries_per_corpus**: Number of queries to generate per round, `-1` will run indefinitely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7cd538ad-b80b-47dc-9c8b-f0c634efbc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying Query 0\n",
      "Trying Query 1\n",
      "Error occurred: Expecting ',' delimiter: line 11 column 488 (char 995)\n",
      "Trying Query 1\n",
      "Trying Query 2\n",
      "Trying Query 3\n",
      "Trying Query 4\n"
     ]
    }
   ],
   "source": [
    "synthetic_pipeline.generate_queries_and_excerpts(approximate_excerpts=True, \n",
    "                                         num_rounds=1, \n",
    "                                         queries_per_corpus=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e3f365-4097-4165-8a82-0cd502ed37b2",
   "metadata": {},
   "source": [
    "**Filter Poor Excerpts**\n",
    "\n",
    "This method filters out questions where any of the references aren't sufficiently similar to the question semantically. This is done by embedding the question and reference(s) and comparing both through semantic similarity. Under a certain threshold, defaulting to `0.36` the line is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "95dd7591-8bc3-42ba-a5bd-46283298b969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: ./domain_specific/nvidia_10k.txt - Removed 29 .\n"
     ]
    }
   ],
   "source": [
    "synthetic_pipeline.filter_poor_excerpts(threshold=0.36)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f0c398-93f0-4f68-b019-d972c5488870",
   "metadata": {},
   "source": [
    "**Remove Duplicates**\n",
    "\n",
    "This method looks then at the questions generated themselves, first removing all of the exact duplicates then creating a similarity matrix comparing every question to every other, again by embedding. It then applies a greedy algorithm to remove similar questions by:\n",
    "1. Keeping the first question\n",
    "2. Removing any later questions that are too similar above a certain threshold. `0.78` by default.\n",
    "3. Move to the next question and repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "35b79312-4794-49c5-925e-1c81b3815206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: ./domain_specific/nvidia_10k.txt - Removed 10 .\n"
     ]
    }
   ],
   "source": [
    "synthetic_pipeline.filter_duplicates(threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d1d0ec-f9a2-4ed5-955d-77f9cf668053",
   "metadata": {},
   "source": [
    "We now have a cleaned dataset of relevant and unique questions as our synthetic evaluation dataset. We initially generated 105, but reduced down to 66 through our filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d45f9d7b-5884-4b6e-90b5-c23bc57e79bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>references</th>\n",
       "      <th>corpus_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>What are the conditions necessary for recogniz...</td>\n",
       "      <td>[{\"content\": \"benefits during the period.\\nWe ...</td>\n",
       "      <td>./domain_specific/nvidia_10k.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>What subsidiaries does NVIDIA Corporation own?</td>\n",
       "      <td>[{\"content\": \"a significant subsidiary.\\nSubsi...</td>\n",
       "      <td>./domain_specific/nvidia_10k.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>What are the significant changes in Other Inco...</td>\n",
       "      <td>[{\"content\": \"prepayment provided at\\nsigning....</td>\n",
       "      <td>./domain_specific/nvidia_10k.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>What are the recent changes in the share repur...</td>\n",
       "      <td>[{\"content\": \"Shareholders\\u2019 Equity\\nCapit...</td>\n",
       "      <td>./domain_specific/nvidia_10k.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>What does NVIDIA's full-stack computing infras...</td>\n",
       "      <td>[{\"content\": \"offerings that are reshaping ind...</td>\n",
       "      <td>./domain_specific/nvidia_10k.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "61  What are the conditions necessary for recogniz...   \n",
       "62     What subsidiaries does NVIDIA Corporation own?   \n",
       "63  What are the significant changes in Other Inco...   \n",
       "64  What are the recent changes in the share repur...   \n",
       "65  What does NVIDIA's full-stack computing infras...   \n",
       "\n",
       "                                           references  \\\n",
       "61  [{\"content\": \"benefits during the period.\\nWe ...   \n",
       "62  [{\"content\": \"a significant subsidiary.\\nSubsi...   \n",
       "63  [{\"content\": \"prepayment provided at\\nsigning....   \n",
       "64  [{\"content\": \"Shareholders\\u2019 Equity\\nCapit...   \n",
       "65  [{\"content\": \"offerings that are reshaping ind...   \n",
       "\n",
       "                           corpus_id  \n",
       "61  ./domain_specific/nvidia_10k.txt  \n",
       "62  ./domain_specific/nvidia_10k.txt  \n",
       "63  ./domain_specific/nvidia_10k.txt  \n",
       "64  ./domain_specific/nvidia_10k.txt  \n",
       "65  ./domain_specific/nvidia_10k.txt  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_df = pd.read_csv(csv_path)\n",
    "synthetic_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f6c24-8b16-486c-8249-184cf7cb80ec",
   "metadata": {},
   "source": [
    "## Running Evaluations\n",
    "\n",
    "We can now employ the same techniques as earlier to run evaluations, this time however using our newly created dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1e4f5318-bd05-4018-9cd9-f60e7798dcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.7668 ± 0.3774\n",
      "Precision: 0.0455 ± 0.0413\n",
      "Precision Ω: 0.1769 ± 0.1181\n",
      "IoU: 0.0453 ± 0.0411\n"
     ]
    }
   ],
   "source": [
    "# Define Chunking Approach\n",
    "sentence_chunker = SentenceChunker(sentences_per_chunk = 10)\n",
    "\n",
    "# Define OpenAI Embedding Model\n",
    "default_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model_name=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "# Run the Eval With Chunker and Embedding Model on our Synthetic Dataset\n",
    "synth_results = synthetic_pipeline.run(sentence_chunker, default_ef)\n",
    "\n",
    "# Display our Results\n",
    "print_metrics(synth_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d850633-3b0b-46c3-80c2-6629abaf0c25",
   "metadata": {},
   "source": [
    "Compared to our earlier results:\n",
    "\n",
    "```\n",
    "Prior Recall: 0.8703 ± 0.3216\n",
    "Prior Precision: 0.0370 ± 0.0303\n",
    "Prior Precision Ω: 0.1674 ± 0.1084\n",
    "Prior IoU: 0.0370 ± 0.0303\n",
    "```\n",
    "We see a lower recall, but higher precision, precision Ω, and IoU!  \n",
    "Let's also perform the same sweep and observe the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "34c5f792-6cb8-4125-812b-8d47f4f7c20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iou_mean</th>\n",
       "      <th>iou_std</th>\n",
       "      <th>recall_mean</th>\n",
       "      <th>recall_std</th>\n",
       "      <th>precision_omega_mean</th>\n",
       "      <th>precision_omega_std</th>\n",
       "      <th>precision_mean</th>\n",
       "      <th>precision_std</th>\n",
       "      <th>chunker</th>\n",
       "      <th>chunk_size</th>\n",
       "      <th>embedding_function</th>\n",
       "      <th>config</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.065792</td>\n",
       "      <td>0.071120</td>\n",
       "      <td>0.639653</td>\n",
       "      <td>0.410139</td>\n",
       "      <td>0.264383</td>\n",
       "      <td>0.148847</td>\n",
       "      <td>0.067492</td>\n",
       "      <td>0.072384</td>\n",
       "      <td>SentenceChunker</td>\n",
       "      <td>5</td>\n",
       "      <td>SentenceTransformerEmbeddingFunction</td>\n",
       "      <td>SentenceChunker_5_SentenceTransformerEmbedding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.077645</td>\n",
       "      <td>0.070578</td>\n",
       "      <td>0.767041</td>\n",
       "      <td>0.332649</td>\n",
       "      <td>0.264383</td>\n",
       "      <td>0.148847</td>\n",
       "      <td>0.079137</td>\n",
       "      <td>0.071506</td>\n",
       "      <td>SentenceChunker</td>\n",
       "      <td>5</td>\n",
       "      <td>OpenAIEmbeddingFunction</td>\n",
       "      <td>SentenceChunker_5_OpenAIEmbeddingFunction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.035074</td>\n",
       "      <td>0.042812</td>\n",
       "      <td>0.633313</td>\n",
       "      <td>0.443139</td>\n",
       "      <td>0.176874</td>\n",
       "      <td>0.118116</td>\n",
       "      <td>0.035287</td>\n",
       "      <td>0.042903</td>\n",
       "      <td>SentenceChunker</td>\n",
       "      <td>10</td>\n",
       "      <td>SentenceTransformerEmbeddingFunction</td>\n",
       "      <td>SentenceChunker_10_SentenceTransformerEmbeddin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.045239</td>\n",
       "      <td>0.041125</td>\n",
       "      <td>0.766759</td>\n",
       "      <td>0.377373</td>\n",
       "      <td>0.176874</td>\n",
       "      <td>0.118116</td>\n",
       "      <td>0.045496</td>\n",
       "      <td>0.041270</td>\n",
       "      <td>SentenceChunker</td>\n",
       "      <td>10</td>\n",
       "      <td>OpenAIEmbeddingFunction</td>\n",
       "      <td>SentenceChunker_10_OpenAIEmbeddingFunction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022789</td>\n",
       "      <td>0.030471</td>\n",
       "      <td>0.525709</td>\n",
       "      <td>0.472539</td>\n",
       "      <td>0.131036</td>\n",
       "      <td>0.089597</td>\n",
       "      <td>0.022841</td>\n",
       "      <td>0.030490</td>\n",
       "      <td>SentenceChunker</td>\n",
       "      <td>15</td>\n",
       "      <td>SentenceTransformerEmbeddingFunction</td>\n",
       "      <td>SentenceChunker_15_SentenceTransformerEmbeddin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.032013</td>\n",
       "      <td>0.030004</td>\n",
       "      <td>0.766126</td>\n",
       "      <td>0.400795</td>\n",
       "      <td>0.131036</td>\n",
       "      <td>0.089597</td>\n",
       "      <td>0.032084</td>\n",
       "      <td>0.030004</td>\n",
       "      <td>SentenceChunker</td>\n",
       "      <td>15</td>\n",
       "      <td>OpenAIEmbeddingFunction</td>\n",
       "      <td>SentenceChunker_15_OpenAIEmbeddingFunction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.016428</td>\n",
       "      <td>0.019835</td>\n",
       "      <td>0.563793</td>\n",
       "      <td>0.466783</td>\n",
       "      <td>0.108410</td>\n",
       "      <td>0.078149</td>\n",
       "      <td>0.016456</td>\n",
       "      <td>0.019848</td>\n",
       "      <td>SentenceChunker</td>\n",
       "      <td>20</td>\n",
       "      <td>SentenceTransformerEmbeddingFunction</td>\n",
       "      <td>SentenceChunker_20_SentenceTransformerEmbeddin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.022750</td>\n",
       "      <td>0.022281</td>\n",
       "      <td>0.685653</td>\n",
       "      <td>0.433153</td>\n",
       "      <td>0.108410</td>\n",
       "      <td>0.078149</td>\n",
       "      <td>0.022793</td>\n",
       "      <td>0.022305</td>\n",
       "      <td>SentenceChunker</td>\n",
       "      <td>20</td>\n",
       "      <td>OpenAIEmbeddingFunction</td>\n",
       "      <td>SentenceChunker_20_OpenAIEmbeddingFunction</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iou_mean   iou_std  recall_mean  recall_std  precision_omega_mean  \\\n",
       "0  0.065792  0.071120     0.639653    0.410139              0.264383   \n",
       "1  0.077645  0.070578     0.767041    0.332649              0.264383   \n",
       "2  0.035074  0.042812     0.633313    0.443139              0.176874   \n",
       "3  0.045239  0.041125     0.766759    0.377373              0.176874   \n",
       "4  0.022789  0.030471     0.525709    0.472539              0.131036   \n",
       "5  0.032013  0.030004     0.766126    0.400795              0.131036   \n",
       "6  0.016428  0.019835     0.563793    0.466783              0.108410   \n",
       "7  0.022750  0.022281     0.685653    0.433153              0.108410   \n",
       "\n",
       "   precision_omega_std  precision_mean  precision_std          chunker  \\\n",
       "0             0.148847        0.067492       0.072384  SentenceChunker   \n",
       "1             0.148847        0.079137       0.071506  SentenceChunker   \n",
       "2             0.118116        0.035287       0.042903  SentenceChunker   \n",
       "3             0.118116        0.045496       0.041270  SentenceChunker   \n",
       "4             0.089597        0.022841       0.030490  SentenceChunker   \n",
       "5             0.089597        0.032084       0.030004  SentenceChunker   \n",
       "6             0.078149        0.016456       0.019848  SentenceChunker   \n",
       "7             0.078149        0.022793       0.022305  SentenceChunker   \n",
       "\n",
       "   chunk_size                    embedding_function  \\\n",
       "0           5  SentenceTransformerEmbeddingFunction   \n",
       "1           5               OpenAIEmbeddingFunction   \n",
       "2          10  SentenceTransformerEmbeddingFunction   \n",
       "3          10               OpenAIEmbeddingFunction   \n",
       "4          15  SentenceTransformerEmbeddingFunction   \n",
       "5          15               OpenAIEmbeddingFunction   \n",
       "6          20  SentenceTransformerEmbeddingFunction   \n",
       "7          20               OpenAIEmbeddingFunction   \n",
       "\n",
       "                                              config  \n",
       "0  SentenceChunker_5_SentenceTransformerEmbedding...  \n",
       "1          SentenceChunker_5_OpenAIEmbeddingFunction  \n",
       "2  SentenceChunker_10_SentenceTransformerEmbeddin...  \n",
       "3         SentenceChunker_10_OpenAIEmbeddingFunction  \n",
       "4  SentenceChunker_15_SentenceTransformerEmbeddin...  \n",
       "5         SentenceChunker_15_OpenAIEmbeddingFunction  \n",
       "6  SentenceChunker_20_SentenceTransformerEmbeddin...  \n",
       "7         SentenceChunker_20_OpenAIEmbeddingFunction  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defining our Configurations\n",
    "chunkers = [\n",
    "    SentenceChunker(sentences_per_chunk = 5),\n",
    "    SentenceChunker(sentences_per_chunk = 10),\n",
    "    SentenceChunker(sentences_per_chunk = 15),\n",
    "    SentenceChunker(sentences_per_chunk = 20),\n",
    "]\n",
    "\n",
    "# Defining our Embedding Functions\n",
    "embedders = [\n",
    "    embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\"),\n",
    "    embedding_functions.OpenAIEmbeddingFunction(api_key=os.environ[\"OPENAI_API_KEY\"], model_name=\"text-embedding-3-large\"),\n",
    "]\n",
    "\n",
    "# Initialize Results Storage\n",
    "synth_results = []\n",
    "\n",
    "# Helper Function\n",
    "def get_config_name(chunker, ef):\n",
    "    chunk_size = chunker.sentences_per_chunk if hasattr(chunker, 'sentences_per_chunk') else 0\n",
    "    ef_name = ef.model_name if hasattr(ef, 'model_name') else ef.__class__.__name__\n",
    "    return f\"{chunker.__class__.__name__}_{chunk_size}_{ef_name}\"\n",
    "\n",
    "# Progress tracking\n",
    "total_combinations = len(chunkers) * len(embedders)\n",
    "current_combination = 0\n",
    "\n",
    "# Run evaluation sweep\n",
    "for chunker in chunkers:\n",
    "    for ef in embedders:\n",
    "        current_combination += 1\n",
    "        try:\n",
    "            print(f\"Evaluating combination {current_combination}/{total_combinations}:\")\n",
    "            print(f\"  Chunker: {chunker.__class__.__name__} (size: {chunker.sentences_per_chunk})\")\n",
    "            print(f\"  Embedding: {ef.model_name if hasattr(ef, 'model_name') else ef.__class__.__name__}\")\n",
    "            \n",
    "            # Run evaluation\n",
    "            result = synthetic_pipeline.run(chunker, ef, retrieve=5)\n",
    "            \n",
    "            # Clean up and store results\n",
    "            if 'corpora_scores' in result:\n",
    "                del result['corpora_scores']\n",
    "            \n",
    "            # Add configuration identifiers\n",
    "            result['chunker'] = chunker.__class__.__name__\n",
    "            result['chunk_size'] = chunker.sentences_per_chunk\n",
    "            result['embedding_function'] = ef.model_name if hasattr(ef, 'model_name') else ef.__class__.__name__\n",
    "            result['config'] = get_config_name(chunker, ef)\n",
    "            \n",
    "            synth_results.append(result)\n",
    "            clear_output(wait=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Error Handling Just in Case\n",
    "            print(f\"Error in combination {current_combination}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Create final DataFrame and display\n",
    "synth_df = pd.DataFrame(synth_results)\n",
    "print(\"\\nFinal Results:\")\n",
    "display(synth_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5b0135-87e5-4ff1-865c-fcc249feb673",
   "metadata": {},
   "source": [
    "<img src=\"./media_2/synth_sweep.png\" width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7fcc06-67d2-474c-b37e-db9509556d9d",
   "metadata": {},
   "source": [
    "---\n",
    "# Discussion\n",
    "\n",
    "Chroma's research framework provides a powerful toolset for evaluating and optimizing RAG systems through careful analysis of chunking and embedding strategies. Through our experiments, we've uncovered several key insights:\n",
    "\n",
    "1. **Chunking Strategy Impact**: Our evaluation demonstrated how different chunking approaches can dramatically affect retrieval performance. Smaller chunks (5 sentences) consistently showed higher precision but at the cost of potentially fragmenting related content, while larger chunks (15-20 sentences) achieved better recall but with more noise.\n",
    "\n",
    "2. **Embedding Model Comparison**: The experiments clearly showed the performance gap between state-of-the-art models (OpenAI's text-embedding-3-large) and lighter-weight alternatives (SentenceTransformer). This helps quantify the tradeoff between cost/speed and performance when choosing embedding models.\n",
    "\n",
    "3. **Domain Adaptation**: The synthetic dataset generation pipeline revealed how general-purpose chunking strategies might need adjustment for specific document types. The financial documentation from NVIDIA's 10-K showed different optimal chunking parameters compared to the general evaluation corpus, highlighting the importance of domain-specific tuning.\n",
    "\n",
    "4. **Metric Tradeoffs**: Throughout our evaluation, we observed the fundamental tension between precision and recall, with IoU providing a balanced perspective on overall performance. These metrics help guide practical decisions about chunk size and strategy based on specific use case requirements.\n",
    "\n",
    "This framework provides a systematic approach to developing and testing RAG systems, allowing practitioners to make data-driven decisions about their text processing pipeline. Future work might explore additional chunking strategies optimized for specific document types, or investigate how different preprocessing steps could improve retrieval performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff9ac06-a4d8-416f-a678-d0c57c890c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (evals)",
   "language": "python",
   "name": "evals"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
