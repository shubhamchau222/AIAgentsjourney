{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, GoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = r\"D:\\common_credentials\\.env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"]=os.getenv(\"LANGSMITH_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGSMITH_PROJECT\")\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"]=os.getenv(\"LANGSMITH_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Groq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are a crucial component of modern natural language processing (NLP) systems, and their importance can be understood from several perspectives:\n",
      "\n",
      "1. **Efficient Processing**: Fast language models enable the rapid processing of large volumes of text data, which is essential for many applications, such as:\n",
      "\t* Sentiment analysis\n",
      "\t* Text classification\n",
      "\t* Named entity recognition\n",
      "\t* Machine translation\n",
      "\t* Chatbots and virtual assistants\n",
      "2. **Real-time Applications**: Fast language models are necessary for real-time applications, where responsiveness is critical, such as:\n",
      "\t* Voice assistants (e.g., Siri, Alexa, Google Assistant)\n",
      "\t* Live chat support\n",
      "\t* Real-time translation\n",
      "\t* Sentiment analysis for social media monitoring\n",
      "3. **Scalability**: Fast language models can handle large amounts of data and scale to meet the demands of:\n",
      "\t* Big data analytics\n",
      "\t* Cloud-based services\n",
      "\t* Enterprise-level applications\n",
      "4. **Improved User Experience**: Fast language models provide a better user experience by:\n",
      "\t* Reducing latency and response times\n",
      "\t* Enabling more accurate and relevant results\n",
      "\t* Supporting more complex and nuanced language understanding\n",
      "5. **Competitive Advantage**: Organizations that leverage fast language models can gain a competitive advantage by:\n",
      "\t* Improving customer engagement and satisfaction\n",
      "\t* Enhancing operational efficiency\n",
      "\t* Unlocking new revenue streams through innovative applications\n",
      "6. **Research and Development**: Fast language models accelerate research and development in NLP, enabling scientists and engineers to:\n",
      "\t* Explore new ideas and techniques\n",
      "\t* Test and validate hypotheses\n",
      "\t* Develop more advanced and sophisticated language models\n",
      "7. **Edge AI and IoT**: Fast language models are essential for edge AI and IoT applications, where:\n",
      "\t* Low-latency and low-power processing are critical\n",
      "\t* Real-time processing and decision-making are required\n",
      "8. **Multilingual Support**: Fast language models can support multiple languages, enabling:\n",
      "\t* Global communication and collaboration\n",
      "\t* Cross-lingual information retrieval and analysis\n",
      "\t* Multilingual customer support and services\n",
      "9. **Explainability and Transparency**: Fast language models can provide insights into their decision-making processes, enabling:\n",
      "\t* Model interpretability and explainability\n",
      "\t* Transparency and trust in AI-driven systems\n",
      "10. **Future-Proofing**: Fast language models are essential for future-proofing NLP systems, as they can:\n",
      "\t* Adapt to evolving language trends and patterns\n",
      "\t* Support emerging applications and use cases\n",
      "\n",
      "In summary, fast language models are vital for efficient, scalable, and responsive NLP systems, and their importance will only continue to grow as language-based applications become increasingly ubiquitous and critical to various industries and aspects of life.\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    #\n",
    "    # Required parameters\n",
    "    #\n",
    "    messages=[\n",
    "        # Set an optional system message. This sets the behavior of the\n",
    "        # assistant and can be used to provide specific instructions for\n",
    "        # how it should behave throughout the conversation.\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"you are a helpful assistant.\"\n",
    "        },\n",
    "        # Set a user message for the assistant to respond to.\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    # The language model which will generate the completion.\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "\n",
    "    #\n",
    "    # Optional parameters\n",
    "    #\n",
    "\n",
    "    # Controls randomness: lowering results in less random completions.\n",
    "    # As the temperature approaches zero, the model will become deterministic\n",
    "    # and repetitive.\n",
    "    temperature=0.5,\n",
    "\n",
    "    # The maximum number of tokens to generate. Requests can use up to\n",
    "    # 32,768 tokens shared between prompt and completion.\n",
    "    max_completion_tokens=1024,\n",
    "\n",
    "    # Controls diversity via nucleus sampling: 0.5 means half of all\n",
    "    # likelihood-weighted options are considered.\n",
    "    top_p=1,\n",
    "\n",
    "    # A stop sequence is a predefined or user-specified text string that\n",
    "    # signals an AI to stop generating content, ensuring its responses\n",
    "    # remain focused and concise. Examples include punctuation marks and\n",
    "    # markers like \"[end]\".\n",
    "    stop=None,\n",
    "\n",
    "    # If set, partial message deltas will be sent.\n",
    "    stream=False,\n",
    ")\n",
    "\n",
    "# Print the completion returned by the LLM.\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "client= Groq(api_key=GROQ_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    response_format={ \"type\": \"json_object\" },\n",
    "    max_tokens=600,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an agent who provide the answer of given quesiton in Json format\"},\n",
    "        {\"role\": \"user\", \"content\": \"Give me the Capital of India and Japan\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n   \"countries\": [\\n       {\\n           \"name\": \"India\",\\n           \"capital\": \"New Delhi\"\\n       },\\n       {\\n           \"name\": \"Japan\",\\n           \"capital\": \"Tokyo\"\\n       }\\n   ]\\n}', role='assistant', function_call=None, tool_calls=None))],\n",
       " 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.choices, len(completion.choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"countries\": [\n",
      "       {\n",
      "           \"name\": \"India\",\n",
      "           \"capital\": \"New Delhi\"\n",
      "       },\n",
      "       {\n",
      "           \"name\": \"Japan\",\n",
      "           \"capital\": \"Tokyo\"\n",
      "       }\n",
      "   ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
