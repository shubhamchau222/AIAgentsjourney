{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHAT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking down text into smaller units called tokens. These tokens can be words, characters, subwords, or symbols, depending on the specific tokenization method used. Tokenization is a fundamental step in natural language processing (NLP) and is crucial for various language-related tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHEN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is typically performed in the following sequence during the preprocessing stage of an NLP pipeline:\n",
    "\n",
    "1. **After Text Cleaning**: \n",
    "   - Once the raw text data is cleaned (removing noise like punctuation, special characters, lowercasing, etc.), tokenization comes next.\n",
    "   \n",
    "2. **Before Vector Creation**: \n",
    "   - Tokenization happens right before converting text into numerical representations. After tokenization, techniques like Bag-of-Words, TF-IDF, or embeddings (Word2Vec, GloVe, transformers) are applied to convert tokens into vectors that models can process.\n",
    "\n",
    "So, tokenization is performed **after text cleaning** and **before vectorization** in the NLP pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHY?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is essential for several reasons:\n",
    "\n",
    "**Input Preparation:** Most NLP models require discrete input units. Tokenization converts raw text into these units.\n",
    "\n",
    "**Granularity Control:** It allows control over the level of granularity in text analysis (e.g., word-level vs. character-level).\n",
    "\n",
    "**Vocabulary Management:** It helps in creating and managing vocabularies for language models.\n",
    "\n",
    "**Feature Extraction:** Tokens serve as features for various NLP tasks.\n",
    "\n",
    "**Dimensionality Reduction:** By breaking text into tokens, we can represent words or subwords as numerical vectors, reducing the dimensionality of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOW?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of tokenization can vary depending on the chosen method, but generally involves these steps:\n",
    "\n",
    "1. **Text Normalization**: This may include converting text to lowercase, removing punctuation, or handling special characters.\n",
    "2. **Boundary Detection**: Identifying where one token ends and another begins. This could be based on whitespace, punctuation, or more complex rules.\n",
    "3. **Token Extraction**: Separating the identified tokens from the original text.\n",
    "4. **Token Processing**: This may involve further processing like stemming, lemmatization, or subword tokenization.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's take a simple sentence: \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "1. Word-level tokenization: \n",
    "   [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "\n",
    "2. Character-level tokenization:\n",
    "   [\"T\", \"h\", \"e\", \" \", \"q\", \"u\", \"i\", \"c\", \"k\", \" \", \"b\", \"r\", \"o\", \"w\", \"n\", \" \", \"f\", \"o\", \"x\", \" \", \"j\", \"u\", \"m\", \"p\", \"s\", \" \", \"o\", \"v\", \"e\", \"r\", \" \", \"t\", \"h\", \"e\", \" \", \"l\", \"a\", \"z\", \"y\", \" \", \"d\", \"o\", \"g\"]\n",
    "\n",
    "3. Subword tokenization (using BPE as an example):\n",
    "   [\"The\", \"quick\", \"brown\", \"fox\", \"jump\", \"##s\", \"over\", \"the\", \"lazy\", \"dog\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIGNIFICANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization plays a crucial role in NLP for several reasons:\n",
    "\n",
    "1. **Language Understanding**: By breaking text into meaningful units, tokenization helps machines better understand and process human language.\n",
    "\n",
    "2. **Vocabulary Size Management**: It allows for control over the vocabulary size, which is crucial for model efficiency and handling out-of-vocabulary words.\n",
    "\n",
    "3. **Cross-lingual Applications**: Some tokenization methods (like BPE) can work across multiple languages, facilitating multilingual NLP models.\n",
    "\n",
    "4. **Handling of Rare Words**: Subword tokenization methods can effectively handle rare words by breaking them into more common subword units.\n",
    "\n",
    "5. **Improved Model Performance**: Proper tokenization can lead to better model performance by providing more meaningful input representations.\n",
    "\n",
    "6. **Consistency**: It ensures consistency in how text is processed, which is essential for reproducible results in NLP tasks.\n",
    "\n",
    "7. **Efficiency**: By converting text into numerical representations, tokenization enables efficient processing of large amounts of text data.\n",
    "\n",
    "8. **Feature Engineering**: Tokens serve as the basis for many feature engineering techniques in NLP, such as bag-of-words, TF-IDF, and n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOKENIZERS TYPES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Working Explanation:\n",
    "A character tokenizer breaks down text into individual characters. Each character, including spaces and punctuation marks, becomes a separate token.\n",
    "\n",
    "#### Example:\n",
    "Input: \"Hello, World!\"\n",
    "Output: [\"H\", \"e\", \"l\", \"l\", \"o\", \",\", \" \", \"W\", \"o\", \"r\", \"l\", \"d\", \"!\"]\n",
    "\n",
    "#### Advantages:\n",
    "1. Simple and straightforward implementation\n",
    "2. No out-of-vocabulary issues\n",
    "3. Useful for tasks that require character-level analysis\n",
    "\n",
    "#### Disadvantages:\n",
    "1. Loses word-level semantics\n",
    "2. Results in very long sequences, which can be computationally expensive\n",
    "3. May not capture higher-level language structures effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world!\"\n",
    "character_tokens = list(text)\n",
    "print(character_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Level Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working Explanation:\n",
    "A word-level tokenizer splits text into individual words. It typically uses spaces and punctuation as delimiters to identify word boundaries.\n",
    "\n",
    "#### Example:\n",
    "Input: \"The quick brown fox jumps over the lazy dog.\"\n",
    "Output: [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "\n",
    "#### Advantages:\n",
    "1. Preserves word-level semantics\n",
    "2. Intuitive and easy to interpret\n",
    "3. Works well for many NLP tasks\n",
    "\n",
    "#### Disadvantages:\n",
    "1. Large vocabulary size, especially for morphologically rich languages\n",
    "2. Cannot handle out-of-vocabulary words\n",
    "3. May struggle with compound words or unconventional spellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whitespace Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working Explanation:\n",
    "A whitespace tokenizer simply splits text on whitespace characters (spaces, tabs, newlines). It's one of the simplest forms of tokenization.\n",
    "\n",
    "#### Example:\n",
    "Input: \"The quick brown fox\\njumps over the lazy dog.\"\n",
    "Output: [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog.\"]\n",
    "\n",
    "#### Advantages:\n",
    "1. Very simple and fast\n",
    "2. Works well for languages with clear word boundaries\n",
    "\n",
    "#### Disadvantages:\n",
    "1. Doesn't handle punctuation well\n",
    "2. May not work properly for languages without clear word boundaries (e.g., Chinese)\n",
    "3. Can't handle contractions or hyphenated words effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', 'world!', 'This', 'is', 'an', 'example', 'text.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "text = \"Hello, world! This is an example text.\"\n",
    "\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SubWord Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SubWord tokenization methods aim to break words into smaller meaningful units, helping to balance vocabulary size and semantic representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Subword Tokenization Methods**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subword tokenization is a crucial technique in Natural Language Processing (NLP) that breaks words into smaller units. This approach helps handle out-of-vocabulary words, reduces vocabulary size, and captures morphological information. Here, we'll discuss four important subword tokenization methods: Byte-Pair Encoding (BPE), WordPiece, Unigram, and SentencePiece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Byte-Pair Encoding (BPE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Working Explanation**\n",
    "\n",
    "BPE starts with a vocabulary of individual characters and iteratively merges the most frequent adjacent pairs of characters or subwords. The process continues until a desired vocabulary size is reached.\n",
    "\n",
    "1. Initialize the vocabulary with individual characters.\n",
    "2. Count the frequency of character pairs in the corpus.\n",
    "3. Merge the most frequent pair and add it to the vocabulary.\n",
    "4. Repeat steps 2-3 until the desired vocabulary size is reached.\n",
    "\n",
    "Example:\n",
    "```\n",
    "Initial words: (\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "Base vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]\n",
    "After merges: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]\n",
    "```\n",
    "\n",
    "##### **Advantages**\n",
    "1. Effective balance between vocabulary size and token expressiveness\n",
    "2. Handles rare words and OOV words well\n",
    "3. Can capture subword semantics\n",
    "4. Works well for multilingual models\n",
    "\n",
    "##### **Disadvantages**\n",
    "1. Can produce unintuitive splits for some words\n",
    "2. Requires training on a corpus\n",
    "3. May not always capture morphological structures effectively\n",
    "\n",
    "##### **Variants**\n",
    "\n",
    "**Byte-level BPE**\n",
    "\n",
    "Used by GPT-2, this variant uses bytes as the base vocabulary, ensuring a fixed base vocabulary size of 256 while being able to tokenize any text without an unknown token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 's', 'a', 'm', 'ple', 'sent', 'ence', '.']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "class BPETokenizer:\n",
    "    \"\"\"\n",
    "    Byte Pair Encoding (BPE) tokenizer using HuggingFace Tokenizers library.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=1000):\n",
    "        self.tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "        self.trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=vocab_size)\n",
    "        self.tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    def train(self, files):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer on a given corpus.\n",
    "        \"\"\"\n",
    "        self.tokenizer.train(files, self.trainer)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize the input text using the trained BPE tokenizer.\n",
    "        \"\"\"\n",
    "        return self.tokenizer.encode(text).tokens\n",
    "\n",
    "# Example usage\n",
    "bpe_tokenizer = BPETokenizer(vocab_size=2000)\n",
    "corpus = [\"res/corpus.txt\"]  # Provide your corpus file here\n",
    "bpe_tokenizer.train(corpus)\n",
    "tokens = bpe_tokenizer.tokenize(\"This is a sample sentence.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. WordPiece**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Working Explanation**\n",
    "\n",
    "WordPiece is similar to BPE but uses a different criterion for merging tokens. Instead of choosing the most frequent pair, it selects the pair that maximizes the likelihood of the training data when added to the vocabulary.\n",
    "\n",
    "1. Initialize the vocabulary with individual characters.\n",
    "2. For each possible merge, calculate the increase in likelihood of the training data.\n",
    "3. Choose the merge that results in the highest increase in likelihood.\n",
    "4. Repeat steps 2-3 until the desired vocabulary size is reached.\n",
    "\n",
    "##### **Advantages**\n",
    "1. Often produces more meaningful subword units\n",
    "2. Balances frequency and usefulness of tokens\n",
    "3. Effective for languages with rich morphology\n",
    "\n",
    "##### **Disadvantages**\n",
    "1. Can be computationally more expensive than BPE\n",
    "2. Still requires a pre-tokenization step for most implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 's', '##a', '##mp', '##le', 'sent', '##ence', '.']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "class WordPieceTokenizerHF:\n",
    "    \"\"\"\n",
    "    WordPiece tokenizer using HuggingFace Tokenizers library.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=1000):\n",
    "        self.tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "        self.trainer = WordPieceTrainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "        self.tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    def train(self, files):\n",
    "        \"\"\"\n",
    "        Train the WordPiece tokenizer on a given corpus.\n",
    "        \"\"\"\n",
    "        self.tokenizer.train(files, self.trainer)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize the input text using the trained WordPiece tokenizer.\n",
    "        \"\"\"\n",
    "        return self.tokenizer.encode(text).tokens\n",
    "\n",
    "# Example usage\n",
    "wordpiece_tokenizer = WordPieceTokenizerHF(vocab_size=2000)\n",
    "corpus = [\"res/corpus.txt\"]  # Provide your corpus file here\n",
    "wordpiece_tokenizer.train(corpus)\n",
    "tokens = wordpiece_tokenizer.tokenize(\"This is a sample sentence.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Unigram**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Working Explanation**\n",
    "\n",
    "Unigram starts with a large vocabulary and iteratively removes tokens to reach the desired vocabulary size.\n",
    "\n",
    "1. Initialize with a large vocabulary (e.g., all pre-tokenized words and common substrings).\n",
    "2. Define a loss function over the training data given the current vocabulary.\n",
    "3. For each symbol, calculate the loss increase if it were removed.\n",
    "4. Remove a percentage of symbols with the lowest loss increase.\n",
    "5. Repeat steps 2-4 until the desired vocabulary size is reached.\n",
    "\n",
    "##### **Advantages**\n",
    "1. Allows for multiple tokenization possibilities, which can improve robustness\n",
    "2. Can find an optimal vocabulary for a given size\n",
    "3. Works well with SentencePiece for language-agnostic tokenization\n",
    "\n",
    "##### **Disadvantages**\n",
    "1. More complex implementation compared to BPE or WordPiece\n",
    "2. May require more computational resources during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'hi', 's', 'i', 's', 'a', 's', 'a', 'm', 'p', 'le', 'sent', 'ence', '.']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import Unigram\n",
    "from tokenizers.trainers import UnigramTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "class UnigramTokenizerHF:\n",
    "    \"\"\"\n",
    "    Unigram tokenizer using HuggingFace Tokenizers library.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size=1000):\n",
    "        self.tokenizer = Tokenizer(Unigram())\n",
    "        self.trainer = UnigramTrainer(vocab_size=vocab_size, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "        self.tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    def train(self, files):\n",
    "        \"\"\"\n",
    "        Train the Unigram tokenizer on a given corpus.\n",
    "        \"\"\"\n",
    "        self.tokenizer.train(files, self.trainer)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize the input text using the trained Unigram tokenizer.\n",
    "        \"\"\"\n",
    "        return self.tokenizer.encode(text).tokens\n",
    "\n",
    "# Example usage\n",
    "unigram_tokenizer = UnigramTokenizerHF(vocab_size=2000)\n",
    "corpus = [\"res/corpus.txt\"]  # Provide your corpus file here\n",
    "unigram_tokenizer.train(corpus)\n",
    "tokens = unigram_tokenizer.tokenize(\"This is a sample sentence.\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4. SentencePiece**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Working Explanation**\n",
    "\n",
    "SentencePiece is not a tokenization algorithm itself, but rather a framework that can use BPE or Unigram algorithms. Its key feature is treating the input as a raw stream, including spaces as part of the token set.\n",
    "\n",
    "1. Treat the input text as a raw stream of characters, including spaces.\n",
    "2. Apply either BPE or Unigram algorithm to this stream.\n",
    "3. Learn a vocabulary that includes space-separated tokens.\n",
    "\n",
    "Example:\n",
    "```\n",
    "Input: \"Hello world\"\n",
    "Tokenized: [\"▁Hello\", \"▁world\"]\n",
    "```\n",
    "(Note: \"▁\" represents the space character)\n",
    "\n",
    "##### **Advantages**\n",
    "1. Language-agnostic: works well for languages without clear word boundaries\n",
    "2. Reversible tokenization: easy to reconstruct the original text\n",
    "3. Consistent tokenization across languages in multilingual models\n",
    "\n",
    "##### **Disadvantages**\n",
    "1. May produce tokens that don't align with linguistic units in some languages\n",
    "2. Can be less intuitive for debugging or analysis compared to word-based tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparison and Use Cases**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **BPE**: Good general-purpose algorithm, widely used (e.g., GPT models)\n",
    "2. **WordPiece**: Effective for morphologically rich languages, used in BERT and related models\n",
    "3. **Unigram**: Offers probabilistic tokenization, good for handling ambiguity\n",
    "4. **SentencePiece**: Excellent for multilingual models and languages without clear word boundaries\n",
    "\n",
    "When choosing a subword tokenization method, consider factors such as the language(s) you're working with, the size of your corpus, computational resources, and the specific requirements of your NLP task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnipostai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
