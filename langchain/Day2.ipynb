{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf8897fd-dfb0-4267-b899-9c6bab8dca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, GoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bdb0db1-3251-4a26-b084-2d1acc342293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_path = r\"D:\\common_credentials\\.env\"\n",
    "load_dotenv(dotenv_path=env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3510be6e-159f-41fd-92e3-8d734b9b0aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='How can I assist you today?' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 36, 'total_tokens': 44, 'completion_time': 0.061578703, 'prompt_time': 0.023836374, 'queue_time': 0.017403141999999996, 'total_time': 0.085415077}, 'model_name': 'llama-3.2-11b-vision-preview', 'system_fingerprint': 'fp_9cb648b966', 'finish_reason': 'stop', 'logprobs': None} id='run-0adf574e-6abd-4ea8-8b65-2d26a2cf85a1-0' usage_metadata={'input_tokens': 36, 'output_tokens': 8, 'total_tokens': 44}\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm= ChatGroq(model=\"llama-3.2-11b-vision-preview\")\n",
    "print(llm.invoke(\"HI\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a0c283-a199-40ca-ab81-e465f2542a30",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efe2d0fc-e6b7-414b-aa95-cc1d346bcb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader= WebBaseLoader(web_path=\"https://python.langchain.com/docs/introduction/\")\n",
    "data= loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9c07a44-5bdb-4d61-bebd-c771984e6493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "splitter= RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=150)\n",
    "docs= splitter.split_documents(data)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc1e9c59-b02a-4afc-a8ed-abdb173878b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embedding_model= GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "len(embedding_model.embed_query(\"Hi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "644b8cb6-b816-4e05-9eb6-cb9fdb560ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x233798f2510>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS, Pinecone\n",
    "vectorstore= FAISS.from_documents(docs, embedding_model)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03c30c19-02a2-49bb-a1bb-b2fa937efff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='6b8117fb-184b-4a77-b21e-0568f76031a6', metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\nLangChain simplifies every stage of the LLM application lifecycle:'),\n",
       "  0.44526535),\n",
       " (Document(id='f5f3dc52-d8ee-43f6-98ff-33b8c163194d', metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content=\"Introductions to all the key parts of LangChain you‚Äôll need to know! Here you'll find high level explanations of all LangChain concepts.\\nFor a deeper dive into LangGraph concepts, check out this page.\\nIntegrations\\u200b\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you're looking to get up and running quickly with chat models, vector stores,\"),\n",
       "  0.49319243)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query= \"what is langchain\"\n",
    "result= vectorstore.similarity_search_with_score(query, k=2)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a83c0a8c-b07f-44de-a77f-16c4f218f266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chain: first=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instructions': 'Return a JSON object.'}, template='Answer the user query.\\n{format_instructions}\\n{query}\\n') middle=[ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002335FDE3990>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000023362B7D0D0>, model_name='llama-3.2-11b-vision-preview', model_kwargs={}, groq_api_key=SecretStr('**********'))] last=JsonOutputParser()\n",
      "-------------------------------\n",
      "\n",
      "{'name': 'LangChain', 'description': 'A Python framework for building and managing chains of AI models, data sources, and other tools.', 'tags': ['AI', 'Machine Learning', 'Natural Language Processing'], 'features': ['Support for various AI models, including LLMs and other models', 'Ability to create chains of models and data sources', 'Support for data storage and retrieval', 'Integration with various APIs and tools'], 'use_cases': [{'title': 'Chatbots and Conversational AI', 'description': 'Use LangChain to build chatbots and conversational AI systems that integrate with multiple models and data sources.'}, {'title': 'Content Generation and Summarization', 'description': 'Use LangChain to create content generation and summarization systems that leverage multiple AI models and data sources.'}, {'title': 'Research and Development', 'description': 'Use LangChain to build and test AI models and systems in a variety of research and development applications.'}]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "\n",
    "output_parser=JsonOutputParser()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "#basic chain \n",
    "\n",
    "chain1= prompt | llm | output_parser\n",
    "print(\"chain:\", chain1)\n",
    "print(\"-------------------------------\\n\")\n",
    "print(chain1.invoke({'query':'what is langchain'}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ec411565-5fb5-4d79-a484-ba981e8bf988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instructions': 'Return a JSON object.'}, template='Answer the user query.\\n{format_instructions}\\n{query}\\n')\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002335FDE3990>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000023362B7D0D0>, model_name='llama-3.2-11b-vision-preview', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| JsonOutputParser()"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "870cc7c7-500d-4880-8645-ea06a725238f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=' Answer the following question based only on provided\\ncontext:\\n<context>\\n{context}\\n</context>'), additional_kwargs={})])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "#this prompt is for generating the ans based on retrieved context\n",
    "generation_prompt= ChatPromptTemplate.from_template(\"\"\" Answer the following question based only on provided\n",
    "context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\"\"\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d2c37bd-83db-4fe8-ba6a-4c3e6d12ab5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content=\" Answer the following question based only on provided\\ncontext:\\n<context>\\nIt's very hot today\\n</context>\", additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_prompt.invoke({\"context\":\"It's very hot today\"}) # sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f8bb071d-5cd7-48b9-829a-d57d53aebf83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=' Answer the following question based only on provided\\ncontext:\\n<context>\\n{context}\\n</context>'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002335FDE3990>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000023362B7D0D0>, model_name='llama-3.2-11b-vision-preview', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain= create_stuff_documents_chain(llm, generation_prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d72540d-f6c9-4451-9ca8-a626e9cb684d",
   "metadata": {},
   "source": [
    "## creating chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c091f51f-919f-45bd-9cb0-dcb58bd893d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever= vectorstore.as_retriever()\n",
    "# retriever.get_relevant_documents(\"what is lanchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "14d41622-f439-4456-9344-d30a7134889a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instructions': 'Return a JSON object.'}, template='Answer the user query.\\n{format_instructions}\\n{query}\\n')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "#this prompt is for generating the ans based on retrieved context\n",
    "generation_prompt= ChatPromptTemplate.from_template(\"\"\" Answer the following question based only on provided\n",
    "context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\"\"\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4123e9c8-f68b-4df3-86aa-25fa3b6f3c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=' Answer the following question based only on provided\\ncontext:\\n<context>\\n{context}\\n</context>'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002335FDE3990>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000023362B7D0D0>, model_name='llama-3.2-11b-vision-preview', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "## this chain will combine the documents get from retriever and pass to the llm through template for final generatin\n",
    "generation_chain= create_stuff_documents_chain(llm, generation_prompt)\n",
    "generation_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f54ab2e-f38e-49fe-a461-234ac3d4385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreval Chain=This chain takes in a user inquiry, which is then passed to the retriever to fetch relevant documents. Those documents (and original inputs) are then passed to an LLM to generate a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7a417d9b-7e0f-4c12-b149-0fe55f344173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000233798F2510>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=' Answer the following question based only on provided\\ncontext:\\n<context>\\n{context}\\n</context>'), additional_kwargs={})])\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002335FDE3990>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000023362B7D0D0>, model_name='llama-3.2-11b-vision-preview', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "# this chain will combine retriever & generation prompt (chatprompt | llm)\n",
    "retrival_chain= create_retrieval_chain(retriever, generation_chain)\n",
    "retrival_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "89d79ca4-a5f9-4218-9109-080f7a28cc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What is langchain', 'context': [Document(id='6b8117fb-184b-4a77-b21e-0568f76031a6', metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\nLangChain simplifies every stage of the LLM application lifecycle:'), Document(id='f5f3dc52-d8ee-43f6-98ff-33b8c163194d', metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content=\"Introductions to all the key parts of LangChain you‚Äôll need to know! Here you'll find high level explanations of all LangChain concepts.\\nFor a deeper dive into LangGraph concepts, check out this page.\\nIntegrations\\u200b\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you're looking to get up and running quickly with chat models, vector stores,\"), Document(id='e48e0e10-1216-4936-97ff-c95d28a56728', metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from'), Document(id='61b81d1f-ed4a-448e-a09a-cd4229a9357d', metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='LangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the integrations page for\\nmore.')], 'answer': \"Based on the provided context, here are the answers to your questions:\\n\\n LangChain is a framework for developing applications powered by large language models (LLMs).\\n\\n LangChain simplifies every stage of the LLM application lifecycle.\\n\\n LangChain's key concepts include:\\n\\n (no specific key concepts are mentioned in the provided context, but it mentions LangGraph concepts which can be found on a different page)\\n\\n LangChain is part of a rich ecosystem of tools that integrate with its framework and build on top of it.\\n\\n LangChain has integrations with hundreds of providers, including:\\n\\n - Chat models\\n - Vector stores\\n - Augmented generation (RAG)\\n - Retrieval\\n - Retrievers\\n - Runnable interface\\n - Streaming\\n - Structured outputs\\n - Testing\\n - String-in, string-out llms\\n - Text splitters\\n - Tokens\\n - Tool calling\\n - Tools\\n - Tracing\\n - Vector stores\\n\\n LangChain's ecosystem includes:\\n\\n - LangSmith\\n - LangGraph\\n\\n LangChain has versions:\\n\\n - v0.3\\n - v0.2\\n - Pydantic compatibility\\n\\n LangChain provides migration guides for those moving from previous versions:\\n\\n - Migrating from v0.0 chains\\n - Migrating from ConstitutionalChain\\n - Migrating from ConversationalChain\\n - Migrating from ConversationalRetrievalChain\\n - Migrating from LLMChain\\n - Migrating from LLMMathChain\"}\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 2.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result= retrival_chain.invoke({\"input\":\"What is langchain\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "933dbb9f-8f6a-4031-adc5-eebf96daec92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What is langgraph?', 'context': [Document(id='19938b6d-3927-4f90-957b-2f7a4cfc7b66', metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='Trace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\nü¶úüï∏Ô∏è LangGraph\\u200b\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\nAdditional resources\\u200b\\nVersions\\u200b\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.'), Document(id='e48e0e10-1216-4936-97ff-c95d28a56728', metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from'), Document(id='c4a905d0-c2d0-4150-971a-db30c12b3f8d', metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations.\\nUse LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\"), Document(id='f5f3dc52-d8ee-43f6-98ff-33b8c163194d', metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content=\"Introductions to all the key parts of LangChain you‚Äôll need to know! Here you'll find high level explanations of all LangChain concepts.\\nFor a deeper dive into LangGraph concepts, check out this page.\\nIntegrations\\u200b\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you're looking to get up and running quickly with chat models, vector stores,\")], 'answer': \"Based on the provided context, I will answer the following questions:\\n\\n1. What is LangGraph used for?\\n\\nLangGraph is used to build stateful, multi-actor applications with Large Language Models (LLMs). It integrates smoothly with LangChain, but can be used without it, and it powers production-grade agents.\\n\\n2. What are some notable companies that trust LangGraph?\\n\\nNotable companies that trust LangGraph include LinkedIn, Uber, Klarna, and GitLab.\\n\\n3. What is the purpose of LangSmith?\\n\\nLangSmith is used to inspect, monitor, and evaluate applications, so that they can be continuously optimized and deployed with confidence.\\n\\n4. What is the LangChain ecosystem?\\n\\nThe LangChain ecosystem is a rich collection of tools that integrate with LangChain's framework and build on top of it.\"}\n",
      "CPU times: total: 93.8 ms\n",
      "Wall time: 1.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result2= retrival_chain.invoke({\"input\":\"What is langgraph?\"})\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a1e25cc3-f38d-449b-9510-df8f40fc35a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided context, I will answer the following questions:\\n\\n1. What is LangGraph used for?\\n\\nLangGraph is used to build stateful, multi-actor applications with Large Language Models (LLMs). It integrates smoothly with LangChain, but can be used without it, and it powers production-grade agents.\\n\\n2. What are some notable companies that trust LangGraph?\\n\\nNotable companies that trust LangGraph include LinkedIn, Uber, Klarna, and GitLab.\\n\\n3. What is the purpose of LangSmith?\\n\\nLangSmith is used to inspect, monitor, and evaluate applications, so that they can be continuously optimized and deployed with confidence.\\n\\n4. What is the LangChain ecosystem?\\n\\nThe LangChain ecosystem is a rich collection of tools that integrate with LangChain's framework and build on top of it.\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0b67be-14d7-483f-a71c-f43955eac4a2",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e96b1-2b63-4612-aa6a-63da64abc027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerieval chain: takes the user query retrieve relevant context & pass to the generation cchain1\n",
    "#create stuff doc chain: combines the llm with generation prompt, receives the docs from retriever"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
