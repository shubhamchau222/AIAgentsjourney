{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and evaluate document extraction ü¶ú‚õìÔ∏è\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langsmith langchain-openai langchain-core langchain-community pydantic python-dotenv openai\n",
    "%pip install --upgrade langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langsmith\n",
    "\n",
    "print(f\"\\nCurrent langsmith version: {langsmith.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the 10-K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "def load_pdf():\n",
    "    loader = PyPDFLoader(\"./aapl.pdf\")\n",
    "    all_text = loader.load()\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langsmith import wrappers, Client\n",
    "from openai import OpenAI\n",
    "openai_client = wrappers.wrap_openai(OpenAI())\n",
    "\n",
    "class UsefulInformation(BaseModel):\n",
    "    products_and_services: list[str] = Field(description=\"A list of products and services provided by the company\")\n",
    "    risk_factors: list[str] = Field(description=\"A list of risk factors described in the document\")\n",
    "    irs_employer_id_number: list[str] = Field(description=\"The IRS Employer Identification Number of the company\")\n",
    "    company_address: list[str] = Field(description=\"The address of the company\")\n",
    "    earnings_per_share_basic: list[str] = Field(description=\"The basic earnings per share of the company\")\n",
    "    net_income: list[str] = Field(description=\"The net income of the company\")\n",
    "\n",
    "def extract_information(doc):\n",
    "    prompt = f\"\"\"\n",
    "    The text below is an excerpt from a 10-K report. You must extract specific information and return it in a structured format.\n",
    "    \n",
    "    CRITICAL INSTRUCTIONS:\n",
    "    1. AVOID DUPLICATES: Never include duplicate items in any list\n",
    "    2. BE CONCISE: Keep each item brief and to the point\n",
    "    3. VALIDATE: Each piece of information must be explicitly stated in the text, do not make assumptions\n",
    "    4. FORMAT: All fields must be lists, even if empty or single item\n",
    "    \n",
    "    Examples of GOOD responses:\n",
    "    - Products: [\"Google Search\", \"Google Cloud\", \"Android\"]\n",
    "    - Address: [\"1600 Amphitheatre Parkway, Mountain View, CA 94043\"]\n",
    "    - Phone: [\"+1 650-253-0000\"]\n",
    "    \n",
    "    Examples of BAD responses (avoid these):\n",
    "    - Duplicates: [\"Google Search\", \"Search by Google\", \"Google Search Engine\"]\n",
    "    - Too verbose: [\"Google Search is a web search engine that allows users to search the World Wide Web...\"]\n",
    "    - Made up data: Do not include information unless explicitly found in the text\n",
    "    \n",
    "    Please extract:\n",
    "    1. Products and Services: List unique products/services (max 10 items)\n",
    "    2. Risk Factors: List unique, critical risks (max 10 items)\n",
    "    3. IRS Employer ID Number: List any EIN found\n",
    "    4. Company Address: List primary address of the company\n",
    "    5. Earnings Per Share (Basic): List basic EPS figure\n",
    "    6. Net Income: List net income figure\n",
    "\n",
    "    Text from the 10-K report:\n",
    "    {doc}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai_client.beta.chat.completions.parse(\n",
    "        model=\"o1-2024-12-17\",\n",
    "        messages=[\n",
    "            { \"role\": \"user\", \"content\": prompt },\n",
    "        ],\n",
    "        response_format=UsefulInformation\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error in structured output LLM call: {str(e)}\")\n",
    "        print(f\"Error type: {type(e)}\")\n",
    "        return UsefulInformation(\n",
    "            products_and_services=[],\n",
    "            risk_factors=[],\n",
    "            irs_employer_id_number=[],\n",
    "            company_address=[],\n",
    "            earnings_per_share_basic=[],\n",
    "            net_income=[]\n",
    "        )\n",
    "\n",
    "def process_all_docs():\n",
    "    all_text =  load_pdf()\n",
    "    results =  extract_information(all_text)\n",
    "    print(\"processed all docs...\")\n",
    "    return results\n",
    "\n",
    "aggregated_info = process_all_docs()\n",
    "print(aggregated_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./extraction-eval.png\" alt=\"extraction-eval\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load existing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"10-k extraction\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define application logic to be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "client = Client()\n",
    "\n",
    "@traceable\n",
    "def target(inputs: dict) -> dict:\n",
    "    response = openai_client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            { \"role\": \"user\", \"content\": inputs[\"input\"][0][\"content\"] },\n",
    "        ],\n",
    "        response_format=UsefulInformation\n",
    "    )\n",
    "    return { \"response\": response.choices[0].message.content }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def format_objects_for_llm_judge(obj1, obj2):\n",
    "    \"\"\"Formats two objects into natural language for easier LLM comparison.\"\"\"\n",
    "    def format_single_object(obj, object_name):\n",
    "        if isinstance(obj, str):\n",
    "            obj = json.loads(obj)\n",
    "        formatted_sections = []\n",
    "        formatted_sections.append(f\"\\n{object_name} contains the following information:\")\n",
    "        sorted_keys = sorted(obj.keys())\n",
    "        for key in sorted_keys:\n",
    "            values = obj[key]\n",
    "            readable_key = key.replace('_', ' ').capitalize()\n",
    "            if isinstance(values, list):\n",
    "                if len(values) == 1:\n",
    "                    formatted_sections.append(f\"\\n{readable_key}: {values[0]}\")\n",
    "                else:\n",
    "                    items = '\\n  - '.join(values)\n",
    "                    formatted_sections.append(f\"\\n{readable_key}:\\n  - {items}\")\n",
    "            else:\n",
    "                formatted_sections.append(f\"\\n{readable_key}: {values}\")\n",
    "        \n",
    "        return '\\n'.join(formatted_sections)\n",
    "\n",
    "    object1_text = format_single_object(obj1, \"Actual Output\")\n",
    "    object2_text = format_single_object(obj2, \"Reference Output\")\n",
    "    return [object1_text, object2_text]\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def run_llm_judge(formatted_text):\n",
    "    class Score(BaseModel):\n",
    "        \"\"\"Evaluate how well an extracted output matches a reference ground truth for 10-K document information.\"\"\"\n",
    "        accuracy: bool = Field(\n",
    "            description=(\n",
    "                \"A binary score (0 or 1) that indicates whether the model's extraction adequately matches the reference ground truth. \"\n",
    "                \"Score 1 if the model's output captures the same essential business information as the reference extraction, even if \"\n",
    "                \"expressed differently. The goal is to verify that the model successfully extracted similar key business information \"\n",
    "                \"as found in the reference ground truth, not to ensure identical representation.\"\n",
    "            )\n",
    "        )\n",
    "        reason: str = Field(\n",
    "            description=(\n",
    "                \"An explanation of how well the model's extraction aligns with the reference ground truth. Consider how effectively \"\n",
    "                \"the model captured the same key business information, financial data, and risk factors as the reference output. \"\n",
    "                \"Acknowledge that variations in expression are acceptable as long as the same core information is captured.\"\n",
    "            )\n",
    "        )\n",
    "    response = openai_client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are evaluating how well a model's extraction of 10-K document information matches a reference ground truth output. \"\n",
    "                    \"Your task is to determine if the model successfully captured the same essential business information as the reference, \"\n",
    "                    \"understanding that similar concepts may be expressed differently.\\n\\n\"\n",
    "                    \"Context:\\n\"\n",
    "                    \"- The reference output represents the ground truth extraction from a 10-K document\\n\"\n",
    "                    \"- The model's output is being evaluated against this reference for accuracy and completeness\\n\"\n",
    "                    \"- Both extractions contain key business information like products/services, risk factors, and financial metrics\\n\"\n",
    "                    \"- The goal is to verify the model captured similar information as the reference, not identical expression\\n\\n\"\n",
    "                    \"Evaluation Guidelines:\\n\"\n",
    "                    \"- Score 1 (true) if the model's output:\\n\"\n",
    "                    \"  * Captures the same core business information as the reference\\n\"\n",
    "                    \"  * Identifies similar risk factors, even if described differently\\n\"\n",
    "                    \"  * Extracts matching or equivalent financial metrics\\n\"\n",
    "                    \"  * Contains consistent company identifiers\\n\"\n",
    "                    \"  * May include additional valid information beyond the reference\\n\\n\"\n",
    "                    \"- Score 0 (false) only if the model's output:\\n\"\n",
    "                    \"  * Misses or contradicts critical information from the reference\\n\"\n",
    "                    \"  * Shows fundamental misunderstanding of the business details\\n\"\n",
    "                    \"  * Contains irreconcilable differences in key metrics\\n\"\n",
    "                    \"  * Fails to capture the essential information found in the reference\\n\\n\"\n",
    "                    \"Remember: The reference output is our ground truth. Evaluate how well the model's extraction \"\n",
    "                    \"captures the same essential business information, allowing for variations in expression.\\n\\n\"\n",
    "                    \"Outputs to Evaluate:\\n\"\n",
    "                    f\"- **Model Output:** {formatted_text[0]}\\n\"\n",
    "                    f\"- **Reference Ground Truth:** {formatted_text[1]}\\n\"\n",
    "                )\n",
    "            }\n",
    "        ],\n",
    "        response_format=Score\n",
    "    )\n",
    "    response_object = json.loads(response.choices[0].message.content)\n",
    "    return { \"response\": response_object }\n",
    "\n",
    "@traceable\n",
    "def evaluate_accuracy(outputs: dict, reference_outputs: dict) -> dict:\n",
    "    actual_output = outputs[\"response\"]\n",
    "    expected_output = reference_outputs['output']\n",
    "    formatted_text = format_objects_for_llm_judge(actual_output, expected_output)\n",
    "    object_response = run_llm_judge(formatted_text)[\"response\"]\n",
    "    return { \"key\": \"accuracy\",\n",
    "            \"score\": object_response[\"accuracy\"],\n",
    "            \"reason\": object_response[\"reason\"] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = client.evaluate(\n",
    "    target,\n",
    "    data=\"10-k extraction\",\n",
    "    evaluators=[evaluate_accuracy],\n",
    "    experiment_prefix=\"10-k-extraction-gpt-4o\",\n",
    "    max_concurrency=5,\n",
    "    num_repetitions=3\n",
    ")\n",
    "\n",
    "experiment_results.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
