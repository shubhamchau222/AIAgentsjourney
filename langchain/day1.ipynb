{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, GoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **setting env variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = r\"D:\\common_credentials\\.env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"]=os.getenv(\"LANGSMITH_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGSMITH_PROJECT\")\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"]=os.getenv(\"LANGSMITH_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pr-advanced-chess-51', 'pr-advanced-chess-51')"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"LANGSMITH_PROJECT\"), os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **load config yaml file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'googleModels': {'Embedding_models': ['models/embedding-001',\n",
       "   'models/text-embedding-004'],\n",
       "  'Inferencing_Models': {'text_models': ['gemini-pro'],\n",
       "   'multimodels': ['gemini-2.0-flash-exp',\n",
       "    'gemini-1.5-flash',\n",
       "    'gemini-1.5-flash-8b']}},\n",
       " 'GroqModels': {'Inferencing_Models': {'multimodels': ['llama-3.2-90b-vision-preview',\n",
       "    'llama-3.2-11b-vision-preview'],\n",
       "   'reasoning_Models': ['deepseek-r1-distill-llama-70b'],\n",
       "   'text_models': ['distil-whisper-large-v3-en',\n",
       "    'gemma2-9b-it',\n",
       "    'llama-3.3-70b-versatile',\n",
       "    'llama-3.1-8b-instant',\n",
       "    'whisper-large-v3-turbo',\n",
       "    'deepseek-r1-distill-llama-70b']}}}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../config.yml\", \"r\") as yml_file:\n",
    "    yml_config= yaml.safe_load(yml_file)\n",
    "    yml_file.close()\n",
    "yml_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## groq apis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"The image appears to be a visual representation of the floorplan or layout of a large building, possibly an office or school. The image is divided into sections, with each section containing multiple rows of what appear to be rooms or classrooms. The rooms are arranged in a grid-like pattern, with some sections having more rows than others. Some sections have windows, which suggests that these rooms are on the upper levels of the building.\\n\\nThe image also features various architectural elements, such as stairwells, corridors, and possibly even a rooftop or outdoor area. The overall design of the building seems to be functional and efficient, with a focus on maximizing space and providing ample natural light.\\n\\nThe image may also include some additional features, such as doors, windows, and possibly even some decorative elements like railings or trim. The overall aesthetic of the building appears to be modern and sleek, with clean lines and minimal ornamentation.\\n\\nIt's worth noting that the image may not be a actual photograph of a building, but rather a digital rendering or a computer-generated image. This is suggested by the level of detail and the precision of the architectural elements, which are not typically visible in real-world photographs.\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"llama-3.2-11b-vision-preview\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What's in this image?\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/f/f2/LPU-v1-die.jpg\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_completion_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=False,\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The image appears to be a visual representation of the floorplan or layout '\n",
      " 'of a large building, possibly an office or school. The image is divided into '\n",
      " 'sections, with each section containing multiple rows of what appear to be '\n",
      " 'rooms or classrooms. The rooms are arranged in a grid-like pattern, with '\n",
      " 'some sections having more rows than others. Some sections have windows, '\n",
      " 'which suggests that these rooms are on the upper levels of the building.\\n'\n",
      " '\\n'\n",
      " 'The image also features various architectural elements, such as stairwells, '\n",
      " 'corridors, and possibly even a rooftop or outdoor area. The overall design '\n",
      " 'of the building seems to be functional and efficient, with a focus on '\n",
      " 'maximizing space and providing ample natural light.\\n'\n",
      " '\\n'\n",
      " 'The image may also include some additional features, such as doors, windows, '\n",
      " 'and possibly even some decorative elements like railings or trim. The '\n",
      " 'overall aesthetic of the building appears to be modern and sleek, with clean '\n",
      " 'lines and minimal ornamentation.\\n'\n",
      " '\\n'\n",
      " \"It's worth noting that the image may not be a actual photograph of a \"\n",
      " 'building, but rather a digital rendering or a computer-generated image. This '\n",
      " 'is suggested by the level of detail and the precision of the architectural '\n",
      " 'elements, which are not typically visible in real-world photographs.')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groque moodels :  ['distil-whisper-large-v3-en', 'gemma2-9b-it', 'llama-3.3-70b-versatile', 'llama-3.1-8b-instant', 'whisper-large-v3-turbo', 'deepseek-r1-distill-llama-70b']\n"
     ]
    }
   ],
   "source": [
    "print(\"Groque moodels : \", yml_config[\"GroqModels\"][\"Inferencing_Models\"][\"text_models\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"In Gujarat's vibrant land of delight,\\n\"\n",
      " 'The sun shines bright with a warm, golden light.\\n'\n",
      " 'The Gir forests whisper secrets of old,\\n'\n",
      " 'As lions roam free, their stories untold.\\n'\n",
      " 'The temples of Somnath stand tall and grand,\\n'\n",
      " \"A testament to the state's rich, cultural hand.\\n\"\n",
      " 'In Gujarat, tradition and progress entwine.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from pprint import  pprint\n",
    "\n",
    "llm= ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "response= llm.invoke(\"write 7 line poem on Gujarat\")\n",
    "pprint(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['query']\n",
      "\"In the land of Gujarat's golden light,\n",
      "Where the sun dips into the Arabian night,\n",
      "A tapestry woven, rich and bold,\n",
      "A heritage of tales, forever to be told.\n",
      "\n",
      "The lions of Gir, with manes of might,\n",
      "Roam free, a symbol of courage in sight,\n",
      "The temples of Somnath, a testament to the past,\n",
      "Echoes of a history that forever will last.\n",
      "\n",
      "The Sabarmati's gentle flow, a river's song,\n",
      "That whispers secrets, of a freedom long,\n",
      "The ashram's humble walls, where Gandhi's spirit resides,\n",
      "A beacon of hope, where love and peace abide.\n",
      "\n",
      "In the vibrant streets of Ahmedabad's old town,\n",
      "The scent of street food, a culinary crown,\n",
      "The sound of the garba, a rhythmic beat,\n",
      "A celebration of life, where hearts skip a treat.\n",
      "\n",
      "The white salt desert, a surreal landscape wide,\n",
      "The Rann of Kutch, where the moon glows with pride,\n",
      "The folk tales of Saurashtra, a treasure to share,\n",
      "A cultural richness, beyond compare.\n",
      "\n",
      "Gujarat, a land of contrasts, a land of might,\n",
      "Where tradition meets innovation, in the morning light,\n",
      "A land of entrepreneurs, with hearts full of fire,\n",
      "Where the spirit of progress, never tires.\n",
      "\n",
      "In this land of diversity, where languages blend,\n",
      "A unique identity, a true Indian trend,\n",
      "The Gujarati spirit, a flame that burns bright,\n",
      "A shining example, of a people's unwavering light.\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt= ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are an exper poet, who writes deep meaning poem\"),\n",
    "    (\"user\",\"{query}\")]\n",
    ")\n",
    "print(prompt.input_variables)\n",
    "\n",
    "chain = prompt | llm\n",
    "response= chain.invoke({\"query\": \"Gujarat\"})\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Value messages=[SystemMessage(content='You are a helpful AI bot. Your name is Bob.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]\n",
      "\n",
      "Chain : first=ChatPromptTemplate(input_variables=['name', 'user_input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='You are a helpful AI bot. Your name is {name}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Hello, how are you doing?'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"I'm doing well, thanks!\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})]) middle=[] last=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001DAC37A3D10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001DAC37A1E90>, model_name='llama-3.3-70b-versatile', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
      "\n",
      "response content=\"My name is Bob, nice to meet you! I'm here to help with any questions or tasks you might have, so feel free to ask me anything. How can I assist you today?\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 76, 'total_tokens': 116, 'completion_time': 0.201239739, 'prompt_time': 0.020388431, 'queue_time': 0.349035458, 'total_time': 0.22162817}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3884478861', 'finish_reason': 'stop', 'logprobs': None} id='run-63d9bd08-d971-407c-8f16-774df7122f19-0' usage_metadata={'input_tokens': 76, 'output_tokens': 40, 'total_tokens': 116}\n",
      "\n",
      " response content:  My name is Bob, nice to meet you! I'm here to help with any questions or tasks you might have, so feel free to ask me anything. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "prompt_value = template.invoke(\n",
    "    {\n",
    "        \"name\": \"Bob\",\n",
    "        \"user_input\": \"What is your name?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print( \"Prompt Value\" ,prompt_value)\n",
    "\n",
    "chain= template | llm\n",
    "print(\"\\nChain :\", chain)\n",
    "\n",
    "#invoke the chain now\n",
    "response= chain.invoke({\n",
    "        \"name\": \"Bob\",\n",
    "        \"user_input\": \"What is your name?\"\n",
    "    })\n",
    "\n",
    "print(\"\\nresponse\", response)\n",
    "\n",
    "print(\"\\n response content: \", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "My\n",
      " name\n",
      " is\n",
      " Bob\n",
      ",\n",
      " nice\n",
      " to\n",
      " meet\n",
      " you\n",
      "!\n",
      " I\n",
      "'m\n",
      " here\n",
      " to\n",
      " help\n",
      " with\n",
      " any\n",
      " questions\n",
      " or\n",
      " tasks\n",
      " you\n",
      " might\n",
      " have\n",
      ",\n",
      " so\n",
      " feel\n",
      " free\n",
      " to\n",
      " ask\n",
      " me\n",
      " anything\n",
      ".\n",
      "\n",
      "My name is Bob, nice to meet you! I'm here to help with any questions or tasks you might have, so feel free to ask me anything.\n"
     ]
    }
   ],
   "source": [
    "## streming hepls to monitor the tokens and all\n",
    "# treaming allows you to display the LLM's output in real-time, as it's being generated.\n",
    "#  This makes your application feel more interactive and responsive, \n",
    "# as users don't have to wait for the entire response to be generated before seeing any results\n",
    "response_tokens= []\n",
    "for i in chain.stream({\n",
    "        \"name\": \"Bob\",\n",
    "        \"user_input\": \"What is your name?\"\n",
    "    }):\n",
    "    print(i.content)\n",
    "    response_tokens.append(i.content)\n",
    "\n",
    "print(\"\".join(response_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Bob, nice to meet you! I'm here to help with any questions or tasks you might have, so feel free to ask me anything.\n"
     ]
    }
   ],
   "source": [
    "print(\"\".join(response_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['query']\n",
      "मैं आपको गुलज़ार साहब की एक प्रसिद्ध कविता सुनाना चाहता हूँ, जो उर्दू शब्दों में है लेकिन हिंदी फ़ॉन्ट में लिखी हुई है:\n",
      "\n",
      "\n",
      "मुहब्बत करने वाले कम नहीं होते\n",
      "नाम अक्सर लोग बदल लेते हैं\n",
      "वो ख्वाब नहीं है जो ख्वाबों में मिले\n",
      "वो सच है जो ख्वाबों में नहीं मिलता\n",
      "\n",
      "\n",
      "मैंने इस कविता को हिंदी फ़ॉन्ट में लिखा है, लेकिन उर्दू शब्दों का उपयोग किया है। आशा है कि आपको यह पसंद आएगी।\n",
      "\n",
      "\n",
      "यह कविता गुलज़ार साहब की एक प्रसिद्ध रचना है, जो मुहब्बत और सच्चाई के बारे में बात करती है।\n"
     ]
    }
   ],
   "source": [
    "prompt= PromptTemplate.from_template(\"share the poem of {query} in urdu words while font in hindi\")\n",
    "print(prompt.input_variables)\n",
    "\n",
    "chain = prompt | llm\n",
    "response= chain.invoke({\"query\": \"Gulzar\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language', 'query']\n",
      "आपल्याला माहीत आहे की, गुलज़ार साहब हे एक प्रसिद्ध कवी आहेत. त्यांची काही पंक्ती अश्या आहेत:\n",
      "गुलज़ार साहेबांची काव्य पंक्ती:\n",
      "मैंने देखा है कभी तुम्हें \n",
      "मैंने देखा है कभी तुम्हें \n",
      "तुम्हारी आँखों में \n",
      "मैंने देखा है खुद को\n"
     ]
    }
   ],
   "source": [
    "#prompt with multiple variables\n",
    "prompt= PromptTemplate.from_template(\"share the poem of {query} in urdu words in 4 lines while font in {language}\")\n",
    "print(prompt.input_variables)\n",
    "chain = prompt | llm\n",
    "response= chain.invoke({\"query\": \"Gulzar\", \"language\": \"Marathi\"})\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chains**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Generative AI focuses on creating new content, such as images, music, or text, based on patterns learned from existing data.\n",
      "* Agentic AI, on the other hand, emphasizes the development of autonomous agents that can make decisions, take actions, and interact with their environment.\n",
      "* Generative AI is primarily used for creative tasks, like generating art, music, or writing, whereas Agentic AI is used for tasks that require decision-making, problem-solving, and adaptability.\n",
      "* Generative models, such as GANs and VAEs, are commonly used in Generative AI, whereas Agentic AI relies on architectures like reinforcement learning, deep learning, and cognitive architectures.\n",
      "* Generative AI is often evaluated based on the quality, coherence, and diversity of the generated content, whereas Agentic AI is evaluated based on its ability to achieve goals, make optimal decisions, and adapt to changing situations.\n",
      "* Generative AI can be used for applications like content creation, data augmentation, and style transfer, whereas Agentic AI is used for applications like robotics, game playing, and autonomous vehicles.\n",
      "* Generative AI typically requires large amounts of data to learn patterns and generate new content, whereas Agentic AI requires a combination of data, environment interactions, and reward signals to learn and adapt.\n",
      "* Generative AI is often more focused on the creative aspects of AI, whereas Agentic AI is more focused on the cognitive and decision-making aspects of AI.\n",
      "* Generative AI can be used to generate realistic synthetic data, whereas Agentic AI can be used to generate realistic agent behavior, such as human-like decision-making and problem-solving.\n",
      "* The development of Generative AI and Agentic AI often requires different skill sets, with Generative AI requiring expertise in machine learning, computer vision, and natural language processing, and Agentic AI requiring expertise in areas like reinforcement learning, game theory, and cognitive science.\n"
     ]
    }
   ],
   "source": [
    "prompt= PromptTemplate.from_template(\"You are an expert who explain the {topic} in point by point lines, don't provide preamble\")\n",
    "\n",
    "#chain\n",
    "chain=prompt|llm\n",
    "response=chain.invoke({\"topic\":\"Can you tell me something about Genertaive ai vs agentic ai\"})\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Output Parser**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stroutput Parser\n",
    "The StrOutputParser is a fundamental component in the Langchain framework, designed to streamline the output from language models (LLMs) and ChatModels into a usable string format. This parser is particularly useful when dealing with outputs that may vary in structure, such as strings or messages. It ensures that the output is consistent and easy to handle in subsequent processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generative AI and Agentic AI are two distinct types of artificial intelligence that have gained significant attention in recent years. While both types of AI have the potential to revolutionize various industries, they differ fundamentally in their goals, approaches, and applications.\\n\\n**Generative AI:**\\n\\nGenerative AI refers to a type of artificial intelligence that focuses on generating new, original content, such as text, images, music, or videos. The primary goal of generative AI is to create novel, coherent, and often realistic outputs that are similar to those produced by humans. Generative AI models, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), use complex algorithms to learn patterns and structures from large datasets and then generate new samples that resemble the training data.\\n\\nExamples of generative AI include:\\n\\n1. Text-to-text models that generate articles, stories, or chatbot responses.\\n2. Image-to-image models that generate realistic images, such as faces, objects, or scenes.\\n3. Music generation models that create original music compositions.\\n\\n**Agentic AI:**\\n\\nAgentic AI, on the other hand, refers to a type of artificial intelligence that focuses on creating autonomous agents that can interact with their environment, make decisions, and take actions to achieve specific goals. Agentic AI models are designed to perceive their surroundings, reason about the consequences of their actions, and adapt to changing situations. The primary goal of agentic AI is to create intelligent agents that can operate independently, learn from experience, and improve their performance over time.\\n\\nExamples of agentic AI include:\\n\\n1. Robotics and autonomous vehicles that can navigate and interact with their environment.\\n2. Intelligent personal assistants that can understand user requests and perform tasks.\\n3. Game-playing agents that can learn strategies and make decisions to win games.\\n\\n**Key differences:**\\n\\n1. **Goals:** Generative AI aims to create new content, while agentic AI focuses on creating autonomous agents that can interact with their environment.\\n2. **Approach:** Generative AI uses complex algorithms to learn patterns and generate new content, whereas agentic AI uses techniques like reinforcement learning, planning, and decision-making to create autonomous agents.\\n3. **Applications:** Generative AI is often used in creative industries, such as art, music, and writing, while agentic AI is used in areas like robotics, gaming, and autonomous systems.\\n4. **Autonomy:** Agentic AI models are designed to be more autonomous and self-directed, while generative AI models are typically more passive and focused on generating content.\\n\\n**Relationship between Generative and Agentic AI:**\\n\\nWhile generative AI and agentic AI are distinct types of AI, they can also be combined to create more powerful and flexible systems. For example, a generative AI model can be used to create new content, such as text or images, which can then be used as input for an agentic AI model to make decisions or take actions. Conversely, an agentic AI model can use generative AI techniques to create new plans, strategies, or scenarios to achieve its goals.\\n\\nIn summary, generative AI and agentic AI represent two complementary approaches to artificial intelligence, each with its strengths and applications. While generative AI excels at creating new content, agentic AI focuses on creating autonomous agents that can interact with their environment and make decisions. By understanding the differences and relationships between these two types of AI, we can unlock new possibilities for innovation and application in various fields.'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "prompt= PromptTemplate.from_template(\"You are an expert who explain the {topic} e\")\n",
    "\n",
    "#chain\n",
    "chain=prompt| llm| StrOutputParser()\n",
    "response=chain.invoke({\"topic\":\"Can you tell me something about Genertaive ai vs agentic ai\"})\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Langsmith', 'description': 'Langsmith is an AI designed to simulate conversation, answer questions, and provide information on a wide range of topics.', 'key_features': ['Natural Language Processing', 'Conversational Interface', 'Knowledge Base'], 'capabilities': ['Answering user queries', 'Providing definitions and explanations', 'Engaging in conversation']}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "output_parser=JsonOutputParser()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query with key.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "output_parser=JsonOutputParser()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query with 'user ans' key.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JsonOutputParser().get_format_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Json Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Return a JSON object with an `Shubham` key that answers the following question: {question}')\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001DAC37A3D10>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001DAC37A1E90>, model_name='llama-3.3-70b-versatile', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| JsonOutputParser()"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "\n",
    "json_prompt = PromptTemplate.from_template(\n",
    "    \"Return a JSON object with an `Shubham` key that answers the following question: {question}\"\n",
    ")\n",
    "json_parser = SimpleJsonOutputParser()\n",
    "json_chain = json_prompt | llm | json_parser\n",
    "json_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{},\n",
       " {'Shubham': ''},\n",
       " {'Shubham': 'The'},\n",
       " {'Shubham': 'The invention'},\n",
       " {'Shubham': 'The invention of'},\n",
       " {'Shubham': 'The invention of the'},\n",
       " {'Shubham': 'The invention of the microscope'},\n",
       " {'Shubham': 'The invention of the microscope is'},\n",
       " {'Shubham': 'The invention of the microscope is often'},\n",
       " {'Shubham': 'The invention of the microscope is often credited'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zach'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias J'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Jans'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen,'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker,'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 159'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590.'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However,'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans J'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son,'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zach'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias,'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope.'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time,'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved,'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Anton'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Lee'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeu'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwen'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenho'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek,'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek, who'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek, who is'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek, who is often'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek, who is often referred'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek, who is often referred to'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek, who is often referred to as'},\n",
       " {'Shubham': 'The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek, who is often referred to as the'},\n",
       " {'Shubham': \"The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek, who is often referred to as the '\"},\n",
       " {'Shubham': \"The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek, who is often referred to as the 'Father\"},\n",
       " {'Shubham': \"The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek, who is often referred to as the 'Father of\"},\n",
       " {'Shubham': \"The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek, who is often referred to as the 'Father of Micro\"},\n",
       " {'Shubham': \"The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek, who is often referred to as the 'Father of Microbiology\"},\n",
       " {'Shubham': \"The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek, who is often referred to as the 'Father of Microbiology'\"},\n",
       " {'Shubham': \"The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek, who is often referred to as the 'Father of Microbiology' due\"},\n",
       " {'Shubham': \"The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek, who is often referred to as the 'Father of Microbiology' due to\"},\n",
       " {'Shubham': \"The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek, who is often referred to as the 'Father of Microbiology' due to his\"},\n",
       " {'Shubham': \"The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek, who is often referred to as the 'Father of Microbiology' due to his extensive\"},\n",
       " {'Shubham': \"The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek, who is often referred to as the 'Father of Microbiology' due to his extensive research\"},\n",
       " {'Shubham': \"The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek, who is often referred to as the 'Father of Microbiology' due to his extensive research using\"},\n",
       " {'Shubham': \"The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek, who is often referred to as the 'Father of Microbiology' due to his extensive research using micro\"},\n",
       " {'Shubham': \"The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek, who is often referred to as the 'Father of Microbiology' due to his extensive research using microscopes\"},\n",
       " {'Shubham': \"The invention of the microscope is often credited to Zacharias Janssen, a Dutch spectacle maker, in 1590. However, other sources suggest that Hans Jansen and his son, Zacharias, may have worked together to create the first compound microscope. Over time, the design of the microscope has evolved, with significant contributions from other scientists such as Antonie van Leeuwenhoek, who is often referred to as the 'Father of Microbiology' due to his extensive research using microscopes.\"}]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(json_chain.stream({\"question\": \"Who invented the microscope?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SimpleJsonOutputParser().get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Ingestion--From the website we need to scrape the data\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x1dac4e0c4d0>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WebBaseLoader(\"https://python.langchain.com/docs/introduction/\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | 🦜️🔗 LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nIntroduction | 🦜️🔗 LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1💬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem🦜🛠️ LangSmith🦜🕸️ LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyIntroductionOn this pageIntroduction\\nLangChain is a framework for developing applications powered by large language models (LLMs).\\nLangChain simplifies every stage of the LLM application lifecycle:\\n\\nDevelopment: Build your applications using LangChain\\'s open-source components and third-party integrations.\\nUse LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\\n\\n\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the integrations page for\\nmore.\\n\\nSelect chat model:OpenAI▾OpenAIAnthropicAzureGoogleAWSCohereNVIDIAFireworks AIGroqMistral AITogether AIDatabrickspip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o-mini\")\\nmodel.invoke(\"Hello, world!\")\\nnoteThese docs focus on the Python LangChain library. Head here for docs on the JavaScript LangChain library.\\nArchitecture\\u200b\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\nArchitecture page.\\n\\nlangchain-core: Base abstractions for chat models and other components.\\nIntegration packages (e.g. langchain-openai, langchain-anthropic, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\nlangchain: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\nlangchain-community: Third-party integrations that are community maintained.\\nlanggraph: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See LangGraph documentation.\\n\\nGuides\\u200b\\nTutorials\\u200b\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our tutorials section.\\nThis is the best place to get started.\\nThese are the best ones to get started with:\\n\\nBuild a Simple LLM Application\\nBuild a Chatbot\\nBuild an Agent\\nIntroduction to LangGraph\\n\\nExplore the full list of LangChain tutorials here, and check out other LangGraph tutorials here. To learn more about LangGraph, check out our first LangChain Academy course, Introduction to LangGraph, available here.\\nHow-to guides\\u200b\\nHere you’ll find short answers to “How do I….?” types of questions.\\nThese how-to guides don’t cover topics in depth – you’ll find that material in the Tutorials and the API Reference.\\nHowever, these guides will help you quickly accomplish common tasks using chat models,\\nvector stores, and other common LangChain components.\\nCheck out LangGraph-specific how-tos here.\\nConceptual guide\\u200b\\nIntroductions to all the key parts of LangChain you’ll need to know! Here you\\'ll find high level explanations of all LangChain concepts.\\nFor a deeper dive into LangGraph concepts, check out this page.\\nIntegrations\\u200b\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with chat models, vector stores,\\nor other LangChain components from a specific provider, check out our growing list of integrations.\\nAPI reference\\u200b\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\nEcosystem\\u200b\\n🦜🛠️ LangSmith\\u200b\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n🦜🕸️ LangGraph\\u200b\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\nAdditional resources\\u200b\\nVersions\\u200b\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\nSecurity\\u200b\\nRead up on security best practices to make sure you\\'re developing safely with LangChain.\\nContributing\\u200b\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.Edit this pageWas this page helpful?NextTutorialsArchitectureGuidesTutorialsHow-to guidesConceptual guideIntegrationsAPI referenceEcosystem🦜🛠️ LangSmith🦜🕸️ LangGraphAdditional resourcesVersionsSecurityContributingCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | 🦜️🔗 LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='Introduction | 🦜️🔗 LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | 🦜️🔗 LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1💬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | 🦜️🔗 LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | 🦜️🔗 LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content=\"knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | 🦜️🔗 LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | 🦜️🔗 LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | 🦜️🔗 LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | 🦜️🔗 LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | 🦜️🔗 LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem🦜🛠️ LangSmith🦜🕸️ LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | 🦜️🔗 LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyIntroductionOn this pageIntroduction'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | 🦜️🔗 LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\nLangChain simplifies every stage of the LLM application lifecycle:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | 🦜️🔗 LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations.\\nUse LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\\n\\n\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the integrations page for\\nmore.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | 🦜️🔗 LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='Select chat model:OpenAI▾OpenAIAnthropicAzureGoogleAWSCohereNVIDIAFireworks AIGroqMistral AITogether AIDatabrickspip install -qU langchain-openaiimport getpassimport osif not os.environ.get(\"OPENAI_API_KEY\"):  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")from langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o-mini\")\\nmodel.invoke(\"Hello, world!\")\\nnoteThese docs focus on the Python LangChain library. Head here for docs on the JavaScript LangChain library.\\nArchitecture\\u200b\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\nArchitecture page.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | 🦜️🔗 LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content=\"langchain-core: Base abstractions for chat models and other components.\\nIntegration packages (e.g. langchain-openai, langchain-anthropic, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\nlangchain: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\\nlangchain-community: Third-party integrations that are community maintained.\\nlanggraph: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See LangGraph documentation.\\n\\nGuides\\u200b\\nTutorials\\u200b\\nIf you're looking to build something specific or are more of a hands-on learner, check out our tutorials section.\\nThis is the best place to get started.\\nThese are the best ones to get started with:\\n\\nBuild a Simple LLM Application\\nBuild a Chatbot\\nBuild an Agent\\nIntroduction to LangGraph\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | 🦜️🔗 LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content=\"Explore the full list of LangChain tutorials here, and check out other LangGraph tutorials here. To learn more about LangGraph, check out our first LangChain Academy course, Introduction to LangGraph, available here.\\nHow-to guides\\u200b\\nHere you’ll find short answers to “How do I….?” types of questions.\\nThese how-to guides don’t cover topics in depth – you’ll find that material in the Tutorials and the API Reference.\\nHowever, these guides will help you quickly accomplish common tasks using chat models,\\nvector stores, and other common LangChain components.\\nCheck out LangGraph-specific how-tos here.\\nConceptual guide\\u200b\\nIntroductions to all the key parts of LangChain you’ll need to know! Here you'll find high level explanations of all LangChain concepts.\\nFor a deeper dive into LangGraph concepts, check out this page.\\nIntegrations\\u200b\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | 🦜️🔗 LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content=\"For a deeper dive into LangGraph concepts, check out this page.\\nIntegrations\\u200b\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you're looking to get up and running quickly with chat models, vector stores,\\nor other LangChain components from a specific provider, check out our growing list of integrations.\\nAPI reference\\u200b\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\nEcosystem\\u200b\\n🦜🛠️ LangSmith\\u200b\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n🦜🕸️ LangGraph\\u200b\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\nAdditional resources\\u200b\\nVersions\\u200b\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | 🦜️🔗 LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content=\"Additional resources\\u200b\\nVersions\\u200b\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\nSecurity\\u200b\\nRead up on security best practices to make sure you're developing safely with LangChain.\\nContributing\\u200b\\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.Edit this pageWas this page helpful?NextTutorialsArchitectureGuidesTutorialsHow-to guidesConceptual guideIntegrationsAPI referenceEcosystem🦜🛠️ LangSmith🦜🕸️ LangGraphAdditional resourcesVersionsSecurityContributingCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright © 2025 LangChain, Inc.\")]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(documents)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GoogleGenerativeAIEmbeddings(client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001DAD47DA110>, model='models/text-embedding-004', task_type=None, google_api_key=SecretStr('**********'), credentials=None, client_options=None, transport=None, request_options=None)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings= GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x1dac4d96cd0>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstoredb=Chroma.from_documents(documents,embeddings)\n",
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en', 'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | 🦜️🔗 LangChain'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\nLangChain simplifies every stage of the LLM application lifecycle:'),\n",
       " Document(metadata={'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en', 'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | 🦜️🔗 LangChain'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\nLangChain simplifies every stage of the LLM application lifecycle:'),\n",
       " Document(metadata={'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en', 'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | 🦜️🔗 LangChain'}, page_content=\"For a deeper dive into LangGraph concepts, check out this page.\\nIntegrations\\u200b\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you're looking to get up and running quickly with chat models, vector stores,\\nor other LangChain components from a specific provider, check out our growing list of integrations.\\nAPI reference\\u200b\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\nEcosystem\\u200b\\n🦜🛠️ LangSmith\\u200b\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n🦜🕸️ LangGraph\\u200b\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\nAdditional resources\\u200b\\nVersions\\u200b\"),\n",
       " Document(metadata={'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en', 'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | 🦜️🔗 LangChain'}, page_content=\"For a deeper dive into LangGraph concepts, check out this page.\\nIntegrations\\u200b\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you're looking to get up and running quickly with chat models, vector stores,\\nor other LangChain components from a specific provider, check out our growing list of integrations.\\nAPI reference\\u200b\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\nEcosystem\\u200b\\n🦜🛠️ LangSmith\\u200b\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n🦜🕸️ LangGraph\\u200b\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\nAdditional resources\\u200b\\nVersions\\u200b\")]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"Langchain is a framework\"\n",
    "result=vectorstoredb.similarity_search(query)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LangSmith**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://smith.langchain.com\", target=\"_blank\" rel=\"noopener\">LangSmith Client</a>"
      ],
      "text/plain": [
       "Client (API URL: https://api.smith.langchain.com)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client, traceable\n",
    "langsmith_client = Client(api_key=os.getenv('LANGCHAIN_API_KEY'))\n",
    "langsmith_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pr-advanced-chess-51'"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "ename": "LangSmithNotFoundError",
     "evalue": "Project pr-advanced-chess-51 not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLangSmithNotFoundError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[255], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlangsmith_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_runs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLANGSMITH_PROJECT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39murl\n\u001b[0;32m      2\u001b[0m url\n",
      "File \u001b[1;32mc:\\Users\\Dell\\.conda\\envs\\autogen\\Lib\\site-packages\\langsmith\\client.py:2299\u001b[0m, in \u001b[0;36mClient.list_runs\u001b[1;34m(self, project_id, project_name, run_type, trace_id, reference_example_id, query, filter, trace_filter, tree_filter, is_root, parent_run_id, start_time, error, run_ids, select, limit, **kwargs)\u001b[0m\n\u001b[0;32m   2296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(project_name, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2297\u001b[0m         project_name \u001b[38;5;241m=\u001b[39m [project_name]\n\u001b[0;32m   2298\u001b[0m     project_ids\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m-> 2299\u001b[0m         \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_project\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2300\u001b[0m     )\n\u001b[0;32m   2301\u001b[0m default_select \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   2302\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapp_path\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2303\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchild_run_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2329\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrace_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2330\u001b[0m ]\n\u001b[0;32m   2331\u001b[0m select \u001b[38;5;241m=\u001b[39m select \u001b[38;5;129;01mor\u001b[39;00m default_select\n",
      "File \u001b[1;32mc:\\Users\\Dell\\.conda\\envs\\autogen\\Lib\\site-packages\\langsmith\\client.py:2299\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(project_name, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2297\u001b[0m         project_name \u001b[38;5;241m=\u001b[39m [project_name]\n\u001b[0;32m   2298\u001b[0m     project_ids\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m-> 2299\u001b[0m         [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_project\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mid \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m project_name]\n\u001b[0;32m   2300\u001b[0m     )\n\u001b[0;32m   2301\u001b[0m default_select \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   2302\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapp_path\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2303\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchild_run_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2329\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrace_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2330\u001b[0m ]\n\u001b[0;32m   2331\u001b[0m select \u001b[38;5;241m=\u001b[39m select \u001b[38;5;129;01mor\u001b[39;00m default_select\n",
      "File \u001b[1;32mc:\\Users\\Dell\\.conda\\envs\\autogen\\Lib\\site-packages\\langsmith\\utils.py:138\u001b[0m, in \u001b[0;36mxor_args.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m     invalid_group_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(arg_groups[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m invalid_groups]\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExactly one argument in each of the following\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m groups must be defined:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(invalid_group_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m     )\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\.conda\\envs\\autogen\\Lib\\site-packages\\langsmith\\client.py:2947\u001b[0m, in \u001b[0;36mClient.read_project\u001b[1;34m(self, project_id, project_name, include_stats)\u001b[0m\n\u001b[0;32m   2945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m   2946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2947\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithNotFoundError(\n\u001b[0;32m   2948\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProject \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2949\u001b[0m         )\n\u001b[0;32m   2950\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ls_schemas\u001b[38;5;241m.\u001b[39mTracerSessionResult(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresult[\u001b[38;5;241m0\u001b[39m], _host_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_url)\n\u001b[0;32m   2951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ls_schemas\u001b[38;5;241m.\u001b[39mTracerSessionResult(\n\u001b[0;32m   2952\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse\u001b[38;5;241m.\u001b[39mjson(), _host_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_url\n\u001b[0;32m   2953\u001b[0m )\n",
      "\u001b[1;31mLangSmithNotFoundError\u001b[0m: Project pr-advanced-chess-51 not found"
     ]
    }
   ],
   "source": [
    "url = next(langsmith_client.list_runs(project_name=os.getenv(\"LANGSMITH_PROJECT\"))).url\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "@traceable(langsmith_client)\n",
    "def interactwithGroq(question):\n",
    "    groq_client = Groq()\n",
    "    chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {   \"role\": \"system\",\n",
    "            \"content\": \"you are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"{question}\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.5)\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jalgaon is a city located in the state of Maharashtra, India. It is the administrative headquarters of the Jalgaon district and is situated on the northern bank of the Tapi River. Here are some key facts about Jalgaon city:\\n\\n1. **Location**: Jalgaon is located at an altitude of 209 meters (686 feet) above sea level, approximately 420 kilometers (261 miles) northeast of Mumbai, the state capital.\\n2. **Population**: As of the 2011 census, the city has a population of around 460,000 people, making it one of the larger cities in Maharashtra.\\n3. **Economy**: Jalgaon is a significant commercial center, with major industries including textiles, food processing, and manufacturing. The city is also known for its banana cultivation and is often referred to as the \"Banana City of India.\"\\n4. **Tourist attractions**: Jalgaon has several tourist attractions, including:\\n\\t* The Patna Devi temple, a historic temple dedicated to the goddess Durga.\\n\\t* The Omkareshwar temple, a Shiva temple located on the banks of the Tapi River.\\n\\t* The Gandhi Research Foundation, a research center dedicated to the life and work of Mahatma Gandhi.\\n\\t* The Jalgaon Fort, a historic fort that dates back to the 18th century.\\n5. **Education**: Jalgaon is home to several educational institutions, including the North Maharashtra University, which was established in 1990.\\n6. **Transportation**: Jalgaon is well-connected to other parts of the country by road, rail, and air. The city has a railway station that connects to major cities like Mumbai, Delhi, and Bangalore, and the nearest airport is the Jalgaon Airport, which offers flights to Mumbai and other destinations.\\n7. **Culture**: Jalgaon has a rich cultural heritage, with a blend of traditional and modern influences. The city celebrates several festivals throughout the year, including the Ganesh Chaturthi, Diwali, and Navratri.\\n\\nOverall, Jalgaon is a vibrant city with a rich history, culture, and economy. Its strategic location and connectivity make it an important hub for trade, commerce, and tourism in the region.'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response= interactwithGroq(\"Tell me about jalgaon City.\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
